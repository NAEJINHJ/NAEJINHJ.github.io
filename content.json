{"meta":{"title":"NA'rchive","subtitle":null,"description":"Hyejin's log","author":"Hyejin","url":"https://NAEJINHJ.github.com"},"pages":[],"posts":[{"title":"마카롱 주문 챗봇 with Bot Framework","slug":"botframework-chatbot","date":"2019-03-09T06:57:25.514Z","updated":"2019-03-12T14:31:37.037Z","comments":true,"path":"2019/03/09/botframework-chatbot/","link":"","permalink":"https://NAEJINHJ.github.com/2019/03/09/botframework-chatbot/","excerpt":"","text":"[목차] Microsoft Bot Framework Macaroon Bot 주문 하기 FAQ 기능 DB 연결 Plus [Microsoft Bot Framework] Microsoft Bot Framework는 .NET과 Node.js 언어를 지원하며 클라우드 기반의 봇 포털을 통해 대부분의 메신저와 연결이 가능하다 현재 [공식적으로 지원하는 채널]에는 Skype, 웹챗, Office365 Mail, Facebook, GroupMe, Kik, Slack, Telegram, Twiloo(SMS), Direct Line App Integration 등이 있다 채널에 포함된 메신저는 Bot Service에 연결되어 있다 이는 클라우드에서 제공하는 서비스로 아래와 같은 다양한 기능을 제공한다 - 채널과 연결을 항상 유지 - 메시지 중계 - 상태 관리 C#이나 Node.js로 개발한 봇은 웹 서버에 배포하는데, 배포가 끝나면 이후 웹 서버 주소를 Bot Service에 등록하는 과정을 거친다 개인 정보처럼 민감한 대화를 다루는 서비스이므로 보안을 고려해 HTTPS를 지원해야 한다 [Macaroon Bot] Microsoft Bot Framework에서 가장 중요한 단위는 Dialog다 글자 그대로 대화를 나타내는 단위지만 각각의 업무 단위라고 보면 된다 다음은 Macaroon Bot의 구조이다 챗봇을 개발할 때 가장 먼저 고민할 부분은 어떤 기능을 제공할 것인지와 사용자와 대화를 어떻게 이끌어 갈 것인지 시나리오를 결정하는 부분이다 시나리오는 각 기능 단위(Dialog)로 분리해서 설계하며, Dialogs는 여러 계층으로 구성할 수 있고, Dialog 내에서 다른 Dialog를 호출해 넘나드는 것 역시 가능하다 Bot Framework의 기본 진입점인 RootDialog 외의 Dialog는 추가로 생성해야 한다 해당 챗봇에 쓸 대화를 총 4개 개발하려 하는데, 접속하면 주 대화를 시작하고 주문, 배송 확인, FAQ 중 하나를 선택하도록 대화를 이어간다 개발 도구는 다음과 같다 Visual Studio 2017 (Bot Builder V4 SDK Template for Visual Studio) Azure Bot Emulator [주문하기] 사용자의 편리를 위해 메뉴 선택에 이미지, 버튼, 설명을 포함한 카드를 활용했다 사용자가 메뉴를 모두 고르고, 바로 주문을 선택하면 다음과 같이 영수증이 출력된다 [FAQ] FAQ 기능은 QnA Maker(http://qnamaker.ai) 를 사용해 개발하였다 QnA Maker란, Microsoft가 딥러닝 기반으로 텍스트 마이닝 기능을 제공하는 서비스로, 다음 네 단계를 수행한다 1. 학습 데이터의 수집 및 학습 2. 학습 데이터의 테스트 3. 서비스 배포 4. 챗봇과 연동 QnA Maker는 데이터를 딥러닝을 통해 학습했다가, 질문을 던지면 가장 높은 Score가 나오는 항목을 돌려주는 것으로 대답한다 다음은 마카롱봇의 QnA 리스트 중 일부이다 FAQ 기능은 “그만”이 입력되면 종료된다 [Database 연동] 클라우드 환경에서 원할히 동작할 수 있어야 하고, 챗봇과 원할하게 연동될 수 있어야 하므로 Microsoft AZure에서 제공하는 SQL Database를 활용하였다 테이블은 Menus, Items, Orders 총 3개를 생성하였다 Menus Menus는 음식 메뉴를 저장할 테이블이다 Orders Orders는 주문을 저장하는 테이블이다 Items Items는 주문한 상세 내역이 저장되는 테이블이다 [Plus] 자연어 처리 엔진 LUIS를 사용해, 사용자의 의도 &amp; 요소 파악 기능 추가","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"NLP","slug":"Project/NLP","permalink":"https://NAEJINHJ.github.com/categories/Project/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://NAEJINHJ.github.com/tags/NLP/"},{"name":"Bot_Framework","slug":"Bot-Framework","permalink":"https://NAEJINHJ.github.com/tags/Bot-Framework/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://NAEJINHJ.github.com/tags/Chatbot/"}]},{"title":"딥러닝 모델을 통한 챗봇 만들기 : 모델링","slug":"chatbot-modeling","date":"2019-03-07T02:58:59.337Z","updated":"2019-03-07T02:58:49.985Z","comments":true,"path":"2019/03/07/chatbot-modeling/","link":"","permalink":"https://NAEJINHJ.github.com/2019/03/07/chatbot-modeling/","excerpt":"","text":"[목차] 모델 개요 &lt;모델링&gt; configs data model main predict 정리 및 참고 [모델 개요] 챗봇을 만들기 위해 사용할 모델은 시퀀스 두 시퀀스(Sequence to Sequence)다 이름 그대로 시퀀스 형태의 입력값을 시퀀스 형태의 출력으로 만드는 모델이다 즉, 하나의 텍스트 문장이 입력으로 들어오면 하나의 텍스트 문장이 출력된다 해당 모델은 RNN(재귀 순환 신경망)을 기반으로 하며, 크게 Encoder 부분과 Decoder 부분으로 나뉜다 우선 인코더 부분에서 입력값을 받아 정보를 담은 벡터를 만들어 낸다 이후 디코더에서 이 벡터를 활용해 재귀적으로 출력값을 만들어내는 구조이다 그림 좌측의 인코더에서는 각 재귀 순환 신경망의 스텝마다 입력값이 들어가고 있다 이때 입력값은 하나의 단어가 된다 또한 해당 부분의 전체 RNN 마지막 부분에서 contet vector라는 하나의 벡터값이 나온다 (재귀 순환 신경망의 마지막 은닉 상태 벡터값) 이는 인코더 부분의 정보를 요약해 담고 있는 벡터이다 디코더 부분으로 들어가면 이 벡터를 사용해 새롭게 RNN을 시작한다 이 신경망의 각 스텝마다 출력값(하나의 단어)이 하나씩 나온다 해당 부분은 각 스텝에서의 출력값이 다시 다음 스텝으로 들어가는 구조로, 각 스텝의 출력값이 다음 스텝의 입력값으로 사용된다 인코더 부분을 보면 각 신경망의 각 스텝마다 단어가 하나씩 들어가고 있다 각 단어는 임베딩된 벡터로 변환 후, 입력값으로 사용된다 재현 신경망의 경우, 구현 시 고정된 문장 길이를 정해야 한다 고정된 문장 길이보다 입력된 문장의 길이가 짧을 시, PADDING을 수행한다 디코더에서는 최조 입력값으로 START라는 특정 토큰(문장의 시작을 나타내는 토큰)을 사용한다 디코더 역시 해당 단어가 임베딩된 벡터 형태로 입력되고, 각 스텝마다 출력이 나온다 이렇게 나온 출력 단어가 다음 스텝의 입력값으로 사용된다 반복 후, 최종적으로 END라는 토큰이 나오면 문장의 끝으로 간주하는 형태로 학습을 진행한다 데이터 전처리 과정에서 특정 문장 길이로 자른 후 패딩 처리 및 START나 END 등의 각종 토큰을 넣어야 한다 [모델링] 모델 구현은 여러 개의 파이썬 파일로 구현할 것이다 우선, 전체적인 파일 구조를 알아보자 123456789101112131415.├── data_in # 입력되는 데이터가 모여있는 폴더 ├── ChatBotData.csv # 전체 데이터 ├── ChatBotData.csv_short # 축소된 데이터 (테스트 용도) ├── README.md # 데이터 저자 READMD 파일├── data_out # 출력되는 모든 데이터가 모여있는 폴더 ├── vocabularyData.voc # 사전 파일 ├── check_point # check_point 저장 공간├── config.py # 모델 설정값 지정 소스├── data.py # data 전처리 및 모델에 주입되는 data set 만드는 소스├── main.py # 전체적으로 데이터를 불러오고, 모델 실행├── model.py # 시퀀스 투 시퀀스 모델이 들어 있는 소스└── predict.py # 학습된 모델로 챗봇 기능 사용하기 위한 코드 크게 두 개의 폴더와 5개의 파이썬 파일로 구성되어 있다 [config] config 코드에는 전체적으로 각종 파라미터로 사용 될 하이퍼파라미터 값이 키-값 형태로 지정되어 있다 tf.app.flags 기능을 사용하면 하이퍼 파라미터 값을 지정할 수 있으며, 자료형에 따라 DEFINE_(data type) 형태로 지정해두면 된다 123456789101112131415161718192021222324252627282930313233343536373839404142import tensorflow as tf# 주피터에서 커널에 전달하기 위한 프래그 방법tf.app.flags.DEFINE_string('f','','kernel')# 배치 크기tf.app.flags.DEFINE_integer('batch_size',64,'batch size')# 학습 에폭tf.app.flags.DEFINE_integer('train_steps',10000,'train steps')# 드롭아웃 크기tf.app.flags.DEFINE_float('dropout_width',0.5,'dropout width')# 멀티 레이어 크기 (multi rnn)tf.app.flags.DEFINE_integer('layer_size',3,'layer size')# 가중치 크기tf.app.flags.DEFINE_integer('hidden_size',128,'weights size')# 학습률tf.app.flags.DEFINE_float('learning_rate',1e-3,'learning rate')# 데이터 위치tf.app.flags.DEFINE_string('data_path','./data_in/ChatBotData.csv','data path')# 사전 위치tf.app.flags.DEFINE_string('vocabulary_path','./data_out/vocabularyData.voc','vocabulary path')# 체크 포인트 위치tf.app.flags.DEFINE_string('check_point_path','./data_out/check_point','check point path')# 셔플 시드값tf.app.flags.DEFINE_integer('shuffle_seek',1000,'shuffle random seek')# 시퀀스 길이tf.app.flags.DEFINE_integer('max_sequence_length',25,'max sequence length')# 임베딩 크기tf.app.flags.DEFINE_integer('embedding_size',128,'embedding size')# 형태소에 따른 토크나이징 사용 여부tf.app.flags.DEFINE_boolean('tokenize_as_morph',True,'set morph tokenize')# 임베딩 여부 설정tf.app.flags.DEFINE_boolean('embedding',True,'Use Embedding flag')# 멀티 RNN 여부tf.app.flags.DEFINE_boolean('multilayer',True,'Use Multi Rnn Cell')# Define FLAGSDEFINES = tf.app.flags.FLAGS 이 값들은 각 코드 파일에서 다음과 같이 불러온 후 사용한다 12from config import DEFINES [data] 데이터 불러오기, 데이터 전처리 등 데이터와 관련된 모든 기능이 구현되어 있다 필요 라이브러리 import 12345678910import reimport osimport pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitimport enumfrom konlpy.tag import Oktfrom tqdm import tqdmfrom config import DEFINES 먼저, 데이터 처리와 관련해 몇 가지 설정값을 지정한다 정규 표현식에서 사용할 필터와 PADDING, START, END, UNKNOWN 토큰과 해당 토큰들의 인덱스 값을 지정한다 필터의 경우, 정규 표현식 모듈을 사용해 컴파일 한다 (미리 컴파일 시, 패턴 사용할 때 반복적인 컴파일 시간 감축 가능) 1234567891011121314FILTER = \"([~.,!?\\\"':;)(])\"PAD = \"&lt;PADDING&gt;\"STD = \"&lt;START&gt;\"END = \"&lt;END&gt;\"UNK = \"&lt;UNKNOWN&gt;\"PAD_INDEX = 0STD_INDEX = 1END_INDEX = 2UNK_INDEX = 3MARKER = [PAD,STD,END,UNK]CHANGE_FILTER = re.compile(FILTERS) 다음은 데이터를 불러와 학습 데이터와 검증 데이터로 분리하는 함수이다 67 : 33 비율로 분리하였다 123456789def load_data(): data_df = pd.read_csv(DEFINES.data_path,header=0) question, answer = list(data_df['Q']),list(data_df['A']) # 학습 : 검증 = 67 : 33 train_input,eval_input,train_label,eval_label = train_test_split(question,answer, test_size=0.33,random_state=42) return train_input,train_label,eval_input,eval_label 다음은 한글 테스트를 토크나이징하기 위해 형태소로 분리하는 작업을 수행하는 함수이다 해당 함수의 경우, 환경 설정 파일을 통해 사용 여부를 선택할 수 있다 형태소로 분류한 데이터를 받아 morphs 함수를 통해 토크나이징된 리스트 객체를 받고 공백 문자를 기준으로 문자열로 재구성해 반환한다 123456789def prepro_like_morphlized(data): morph_analyzer = Okt() result_data = list() for seq in tqdm(data): morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ',''))) result_data.append(morphlized_seq) return result_data 다음은 인코더에 적용될 입력값을 만드는 전처리 함수이다 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# value : 전처리할 데이터# dictionary : 단어 사전def enc_processing(value,dictionary): # 인덱스 값 가지고 있기 위한 배열 sequences_input_index = [] # 인코딩 되는 문장의 길이 누적 배열 sequences_length = [] # 입력 데이터에 대해 전처리를 진행하는데, # [형태소 기준으로 토크나이징 / 단순히 띄어쓰기 기준으로 토크나이징]에 다라 처리 흐름이 나뉨 if DEFINES.tokenize_as_morph: value = prepro_like_morphlized(value) for sequence in value: # 정규 표현식 라이브러리를 통해 특수 문자 모두 제거 sequence = re.sub(CHANGE_FILTER,\"\",sequence) # 하나의 문장을 인코딩할 때, 가지고 있기 위한 배열 sequence_index = [] # 단어 사전을 이용해 각 단어 -&gt; 단어 인덱스 # 만약 어떤 단어가 단어 사전에 포함되어 있지 않다면 UNK 토큰을 넣음 # (UNK 토큰의 인덱스 값은 2로 설정했음) for word in sequence.split(): # 잘려진 단어가 딕셔너리에 존재하는지 보고 # 그 값을 가져와 sequence_index에 추가 if dictionary.get(word) is not None: sequence_index.extend([dictionary[word]]) # 잘려진 단어가 딕셔너리에 존재하지 않는 경우이므로 # UNK(2)를 넣어 줌 else: sequence_index.extend([dictionary[UNK]]) # 모델에 적용할 최대 길이보다 긴 문장의 경우 자름 if len(sequence_index) &gt; DEFINES.max_sequence_length: sequence_index = sequence_index[:DEFINES.max_sequence_length] sequences_length.append(len(sequence_index)) # 최대 길이보다 짧은 문장의 경우 문장의 뒷부분 패딩 처리 sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]] # 인덱스화 되어있는 값을 sequence_input_index에 추가 sequence_input_index.append(sequence_index) # np.asarray(sequence_input_index) : 앞서 전처리한 데이터 # sequences_length : 패딩 처리 전, 각 문장의 실제 길이를 담고 있는 리스트 # 인덱스화된 일반 배열을 numpy 배열화 # tensorflow 데이터셋에 넣어주기 위한 사전 작업 return np.asarray(sequence_input_index), sequences_length 다음은 디코더 부분에 필요한 전처리 함수를 만들어보자 디코더에는 두 가지 전처리 함수가 사용된다 - 디코더의 입력으로 사용 될 입력값을 만드는 전처리 함수 - 디코더의 결과로 학습을 위해 필요한 라벨, 즉, 타겟값을 만드는 전처리 함수 입력값은 START 토큰이 앞에 들어가 있고, 타깃값은 문장 끝에 END 토큰이 들어가 있어야 한다 먼저 디코더의 입력값을 만드는 함수이다 인코더 입력값 생성 전처리 함수와 한 가지 다른 점은 각 문장의 처음에 START 토큰을 넣어준다는 점이다 12345678910111213141516171819202122232425262728293031323334353637383940# 디코더의 입력으로 사용될 입력값을 만드는 전처리 함수# \"&lt;START&gt;,안녕,&lt;PADDING&gt;\"# value : 데이터 / dictionary : 단어 사전def dec_input_processing(value, dictionary): # 인덱스 값들을 가지고 있는 배열 sequence_output_index = [] # 디코딩 입력이 되는 문장의 길이 누적 배열 sequences_length = [] # 형태소 토크나이징 사용 유무 if DEFINES.tokenize_as_morph: value = prepro_like_morphlized(value) for sequence in value: # 정규화를 사용해 필터에 들어있는 값들을 \"\"로 치환 sequence = re.sub(CHANGE_FILTER,\"\",sequence) # 디코딩 시, 하나의 문장을 가지고 있기 위한 배열 sequence_index = [] # 디코딩 입력의 맨 앞에 START가 넣기 # 스페이스 단위 별로 단어를 가져와 딕셔너리의 값(인덱스) 넣어줌 sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()] # 문장 제한 길이보다 길 경우, 자르기 if len(sequence_index) &gt; DEFINES.max_sequence_length: sequence_index = sequence_index[:DEFINES.max_sequence_length] # 입력 문장 길이 배열에 문장 길이 추가 sequences_length.append(len(sequence_index)) # 문장 제한 길이보다 짧을 경우, 패딩 sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]] # 인덱스화 되어 있는 값을 인덱스값 배열에 추가 sequence_output_index.append(sequence_index) # 전처리한 데이터 / 각 데이터 문장의 실제 길이 리스트 리턴 return np.asarray(sequence_output_index),sequences_length 타깃값을 만드는 전처리 함수 역시 유사하나, 문장의 시작부분에 START 토큰을 넣지 않고 마지막에 END 토큰을 넣는다는 차이점이 있다 1234567891011121314151617181920212223242526272829303132# 디코더의 결과로 학습에 필요한 라벨인 타깃값을 만드는 전처리 함수# ex) \"안녕, &lt;END&gt;,&lt;PADDING&gt;\"# 시작 토큰 넣지 X, 마지막에 종료 토큰def dec_target_processing(value,dictionary): # 인덱스 값을 가지고 있는 누적 배열 sequences_target_index = [] # 형태소 토크나이징 사용 유무 if DEFINES.tokenize_as_morph: value = prepro_like_morphlized(value) for sequence in value: sequence = re.sub(CHANGE_FILTER,\"\",sequence) # 문장에서 스페이스 단위 별로 단어를 가져와 # 딕셔너리의 값인 인덱스를 넣어준다 sequence_index = [dictionary[word] for word in sequence.split()] # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자른다 # 마지막에 END 토큰을 넣어준다 if len(sequence_index) &gt;= DEFINES.max_sequence_length: sequence_index =sequence_index[:DEFINES.max_sequence_length-1] + [dictionary[END]] else: sequence_index += [dictionary[END]] sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]] sequences_target_index.append(sequence_index) # 인덱스화된 일반 배열을 numpy 배열로 변환한다 # 실제 길이를 담고 있는 리스트의 경우 따로 만들지 X return np.asarray(sequences_target_index) 다음으로 살펴 볼 함수는 학습한 모델을 통해 예측할 때 사용하는 함수이다 인덱스 값들을 실제 단어로 바꾸어야 성능 확인이나 실제 챗봇 용도 사용이 가능하기 때문에 인덱스로 이루어진 문장을 실제 단어들의 문자열로 만드는 함수를 정의한다 1234567891011121314151617181920212223# 학습한 모델을 통해 예측할 때 사용하는 함수# index -&gt; string 변경 함수# 예측 결과는 각 단어의 인덱스 벡터로 나올 것def pred2string(value, dictionary): # 텍스트 문장 보관 배열 sentence_string = [] # 단어 사전 사용해 인덱스 데이터 -&gt; 실제 단어 -&gt; 문자열 for v in value: sentence_string = [dictionary[index] for index in v['indexs']] print(sentence_string) answer = \"\" # 패딩과 종료 토큰의 경우 바꾸지 X , 공백으로 처리 for word in sentence_string: if word not in PAD and word not in END: answer += word answer += \" \" # 결과 출력 print(answer) return answer 계속해서 파라미터로 사용하고 있는 단어사전 역시 함수를 통해 직접 만들어야 한다 단어 사전을 만들기 위해서는 데이터를 전처리한 후 단어 리스트로 만드는 함수가 필요하다 1234567891011121314# 단어 사전 생성을 위한# 데이터를 전처리한 후 단어 리스트로 만드는 함수def data_tokenizer(data): # 정규 표현식을 사용해 특수 기호를 모두 제거하고 # 단어들을 기준으로 나눠 # 전체 데이터의 모든 단어를 포함하는 단어 리스트로 만듦 words = [] for sentence in data: sentence = re.sub(CHANGE_FILTER,\"\",sentence) for word in sentence.split(): words.append(word) # 토크나이징 &amp; 정규표현식 통해 만들어진 값 리턴 return [word for word in words if word] 다음은 단어 사전을 만드는 함수이다 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 단어 사전 만드는 함수def load_vocabulary(): vocabulary_list = [] # 사전을 담을 배열 # 경로에 사전 파일이 있다면 불러와서 사용 # 없다면 만드는 구조 # &gt;&gt; 데이터 불러와서 앞서 정의한 함수를 통해 # 데이터를 토크나이징해 단어 리스트로 만듦 if (not(os.path.exists(DEFINES.vocabulary_path))): # 이미 생성된 사전 파일이 존재하지 않으므로 새로 만듦 if (os.path.exists(DEFINES.data_path)): # 데이터가 존재한다면 데이터 불러오기 data_df = pd.read_csv(DEFINES.data_path, encoding=\"utf-8\") # 질문과 답에 대한 열 가져오기 question, anwser = list(data_df['Q']),list(data_df['A']) # 형태소에 따른 토크나이즈 처리 if DEFINES.tokenize_as_morph: question = prepro_like_morphlized(question) answer = prepro_like_morphlized(answer) data = [] # 질문과 답변을 구조가 없는 배열로 만듦 data.extend(question) data.extend(answer) # 토크나이저 처리 부분 words = data_tokenizer(data) # set 데이터 타입을 사용해 중복을 제거한 후, 단어 리스트로 만듦 words = list(set(words)) # MARKER로 사전에 정의한 특정 토큰들을 단어 리스트 앞에 추가 words[:0] = MARKER # 리스트로 만든 사전을 파일로 저장 with open(DEFINES.vocabulary_path,\"w\") as vocabulary_file: for word in words: vocabulary_file.write(word + \"\\n\") # 사전 파일이 존재하면 불러와 배열에 넣어줌 with open(DEFINES.vocabulary_path,\"r\",encoding=\"utf-8\") as vocabulary_file: for line in vocabulary_file: vocabulary_list.append(line.strip()) # 배열의 내용을 딕셔너리 구조로 만듦 (key-value 구조) word2idx, idx2word = make_vocabulary(vocabulary_list) # word2idx : 단어에 대한 인덱스를 가진 딕셔너리 # idx2word : 인덱스에 대한 단어를 가진 딕셔너리 # len(word2idx) = 단어의 개수 return word2idx,idx2word,len(word2idx) 다음은 두개의 딕셔너리 word2idx와 idx2word를 만드는 함수이다 12345678def make_vocabulary(vocabulary_list): # 파라미터로 받은 단어 리스트로 두 개의 딕셔너리를 만듦 # 단어에 대한 인덱스 딕셔너리 생성 word2idx = &#123;word: idx for idx, word in enumerate(vocabulary_list)&#125; # 인덱스에 대한 단어 딕셔너리 생성 idx2word = &#123;idx: word for idx, word in enumerate(vocabulary_list)&#125; return word2idx, idx2word 다음은 tensorflow 모델에 데이터를 적용하기 위한 데이터 입력 함수이다 (Estimator를 이용한 모델에 적용을 위한 함수) 123456789101112131415161718192021222324252627282930313233343536373839# 학습에 들어가 배치 데이터를 만드는 함수# 파라미터 : 인코더에 적용될 입력값, 디코더에 적용될 입력값, 학습 시 디코더에서 사용될 타깃값, 배치 크기def train_input_fn(train_input_enc,train_output_dec,train_target_dec,batch_size): # dataset을 생성하는 부분 # from_tensor_slices : 각각 한 문장으로 자르기 # train_input_enc, train_output_dec, train_target_dec를 # 각각 한 문장으로 나눈다! dataset = tf.data.Dataset.from_tensor_slices((train_input_enc,train_output_dec,train_target_dec)) # 전체 데이터 섞기 dataset = dataset.shuffle(buffer_size==len(train_input_enc)) # 배치 크기를 파라미터로 전달하지 않을 경우, 에러 발생 시킴 assert batch_size is not None, \"train batchSize must not be None\" # 한 문장씩 나눈 것을 배치 사이즈 만큼 묶어 줌 dataset = dataset.batch(batch_size) # 데이터의 각 요소를 변환해 맵으로 구성 dataset = dataset.map(rearrange) # 파라미터로 에폭 수를 전달해 # 에폭 수 만큼 반복하게 함 # 파라미터를 주지 않았으므로 무한히 반복 # 사용자가 손실값이 적당히 떨어졌다고 판단되면 직접 멈추는 것이 가능 dataset = dataset.repeat() # iterator 생성 iterator = dataset.make_one_shot_iterator() # iterator를 통해 다음 항목의 개체를 넘겨 줌 return iterator.get_next()# 평가에 들어가 배치 데이터를 만드는 함수def eval_input_fn(eval_input_enc,eval_output_dec,eval_target_dec,batch_size): dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc,eval_output_dec,eval_target_dec)) dataset = dataset.shuffle(buffer_size==len(eval_input_enc)) assert batch_size is not None, \"eval batchSize must not be None\" dataset = dataset.batch(batch_size) dataset = dataset.map(rearrange) # 평가이므로 반복을 수행하지 X 1회만 동작 dataset = dataset.repeat(1) iterator = dataset.make_one_shot_iterator() return iterator.get_next() 마지막으로 맵 함수를 정의해보자 12345# 파라미터 : 인코더 적용 입력값, 디코더 적용 입력값, 라벨로 학습 시 적용되는 타깃값def rearrange(input,output,target): # 두 개의 입력값을 딕셔너리 형태로 묶음 features = &#123;\"input\":input,\"output\":output&#125; return features, target [model] Sequence to Sequence 모델을 기반으로 만들 것이며, 해당 모델의 중간에 사용되는 신경망으로는 재현 신경망 중, LSTM(Long-Short Term Memory) 모델을 사용하겠다 해당 파일에는 실제로 사용할 모델이 정의되어 있으며, 총 [2개의 함수]가 존재한다 - 모델을 정의한 함수(model) - 한 스텝마다 적용되는 LSTM과 dropout을 합쳐 하나로 모듈화한 함수(make_lstm_cell) 다음은 LSTM과 dropout을 모듈화한 함수이다 12345678910111213141516import tensorflow as tfimport sysfrom config import DEFINESimport numpy as np# mode : 현재 모델이 작동 중인 모드# hiddenSize : LSTM의 은닉 상태 벡터값의 차원# index : 여러 개의 LSTM 스텝을 만들기 때문에, 각 스텝의 인덱스 값def make_lstm_cell(mode, hiddenSize, index): cell = tf.nn.rnn_cell.BasicLSTMCell(hiddenSize,name=\"lstm\"+str(index)) # 모드를 확인해 학습 중이라면, # RNN에 적용시키는 dropout 적용 # 적용할 dropout 혹률은 사전에 정의한 값으로 받음 if mode == tf.estimator.ModeKeys.TRAIN: cell =tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = DEFINES.dropout_width) return cell 본격적으로 모델 함수를 살펴보자 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149# features : 모델 입력 함수를 통해 만든 피처 값 전달# (딕셔너리 형태 : 인코더 입력 &amp; 디코더 입력)# label : 디코더의 타깃값# mode : 모델의 상태 (학습 / 검증 / 평가)# params : 모델에 적용되는 몇 가지 인자 값을 딕셔너리 형태로 전달 받음def model(features, labels, mode, params): # 모델 함수에 적용된 모드가 어떤 상태인지 각각 상수값으로 설정 TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT # ------------------- 본격적인 모델 시작 ------------------------- # 인코더 / 디코더 구현 이전에 입력값을 모델에 적용할 수 있도록 벡터화 # 두 가지 선택지를 모델 함수에 적용되는 파라미터값을 사용해 선택하도록 함 # 임베딩을 사용하는 경우 / 원-핫 인코딩 사용하는 경우 나눠 각 임베딩 행렬 생성 # ----------------------------인코더----------------------------- # &lt;임베딩을 사용하는 경우&gt; if params['embedding'] == True: # 행렬 초기화 방법 미리 정의 : Xavier(자비어) 방식 initializer = tf.contrib.layers.xavier_initializer() # 임베딩 행렬 만들기 embedding_encoder = tf.get_variable(name=\"embedding_encoder\", shape = [params['vocabulary_length'], params['embedding_size']], dtype = tf.float32, initializer = initializer, trainable = True) # &lt;임베딩을 사용하지 않는 경우&gt; # 임베딩 행렬을 단순 단위 행렬로 정의, 파라미터 설정을 통해 # 학습이 안되도록 함 else: embedding_encoder = tf.eye(num_rows = params['vocabulary_length'], dtype = tf.float32) embedding_encoder = tf.get_variable(name = \"embedding_encoder\", initializer = embedding_encoder, trainable = False) # embedding_lookup 함수를 통해 # 각 단어를 임베딩 벡터로 만듦 embedding_encoder_batch = tf.nn.embedding_lookup(params = embedding_encoder, ids = features['input']) # ----------------------------디코더----------------------------- # &lt;임베딩을 사용하는 경우&gt; if params['embedding'] == True: # 행렬 초기화 방법 미리 정의 initializer = tf.contrib.layers.xavier_initializer() embedding_decoder = tf.get_variable(name = \"embedding_decoder\", shape = [params['vocabulary_length'], params['embedding_size']], dtype = tf.float32, initializer = initializer, trainable = True) # &lt;임베딩을 사용하지 않는 경우&gt; else: embedding_decoder = tf.eye(num_rows = params['vocabulary_length'], dtype = tf.float32) embedding_decoder = tf.get_variable(name = \"embedding_decoder\", initializer = embedding_decoder, trainable = False) embedding_decoder_batch = tf.nn.embedding_lookup(params = embedding_decoder, ids = features['output']) # ========================인코더 부분 구현======================= # LSTM 신경망 생성 # 인자로 전달되는 params의 multilayer 사용 여부에 따라 # 두 가지 방법 중 하나로 구현 됨 with tf.Variable_scope('encoder_scope',reuse=tf.AUTO_REUSE): # multylayer 사용 : 사전에 정의한 make_lstm_cell 함수를 layer 수 만큼 반복해 리스트로 만듦 if params['multilayer'] == True: encoder_cell_list = [make_lstm_cell(mode,params['hidden_size'],i) for i in range(params['layer_size'])] rnn_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list) # multilayer 사용 X : 함수를 한번만 호출 else: rnn_cell = make_lstm_cell(mode,params['hidden_size'],\"\") # 이렇게 만든 LSTM 신경망을 # 입력값과 함께 tensorflow의 dynamic_rnn 함수에 적용 encoder_outputs, encoder_states = tf.nn.dynamic_rnn(cell=rnn_cell, inputs = embedding_encoder_batch,dtype=tf.float32) # ========================디코더 부분 구현======================= with tf.Variable_scope('decoder_scope',reuse=tf.AUTO_REUSE): if params['multilayer'] == True: decoder_cell_list = [make_lstm_cell(mode,params['hidden_size'],i) for i in range(params['layer_size'])] rnn_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list) else: rnn_cell = make_lstm_cell(mode,params['hidden_size'],\"\") # LSTM의 첫 스텝 은닉 상태 벡터 값을 # 인코더의 마지막 스텝 은닉 상태 벡터값(encoder_states)로 초기화 decoder_initial_state = encoder_states decoder_outputs, decoder_states = tf.nn.dynamic_rnn(cell=rnn_cell, inputs = embedding_decoder_batch, initial_state = decoder_initial_state, dtype=tf.float32) # 디코더의 결괏값에 Dense 층 적용 # 결괏값의 차원을 단어의 수만큼 변경하기 위함 logits = tf.keras.layer.Dense(params['vocabulary_length'])(decoder_outputs) # logits으로 출력단어(결괏값)을 뽑아야 함 # Dense 층을 통해 차원을 단어 개수만큼 늘렸기 때문에 # 최댓값을 가지는 위치의 단어를 출력하면 됨 predict = tf.argmax(logits,2) # 현재 [실행 모드가 예측 상태]면 # 손실값 계산, 최적화가 필요 없으므로 바로 return if PREDICT: # 예측값을 딕셔너리 형태로 저장 prediction = &#123; 'indexs' : predict, 'logits' : logits &#125; # Estimator 객체로 return return tf.estimator.EstimatorSpec(mode,predictions = predictions) # [상태가 학습, 평가 일때만 진행되는 부분] # 실제 라벨값과 비교해서 loss 값을 뽑은 후 # 이 값을 이용해 모델을 학습할 것 # 각 라벨 값을 원-핫 인코딩 벡터로 만듦 labels_ = tf.one_hot(labels, params['vocabulary_length']) # 라벨과 모델의 결괏값을 사용해 손실값을 만듦 # softmax_cross_entropy_with_logits_v2 함수 사용 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=labels_)) # 구한 loss 값을 사용해 모델의 성능 측정 # 측정한 정확도는 딕셔너리 형태로 저장 후, # 이후 Estimator의 return 값으로 전달할 것 accuracy = tf.metrics.accuracy(labels = labels, predictions = predict, name=\"acc0p\") # 정확도 측정 # 딕셔너리 형태로 정의 metrics = &#123;'accuracy': accuracy&#125; # 학습 과정의 내용을 저장 하도록 tf.summary.scalar에 정확도 값 제공 tf.summary.scalar('accuracy',accuracy[1]) # [평가 모드인 경우] 함수 리턴 if EVAL: # 손실값과 모델 성능 측정값(정확도 딕셔너리)을 리턴 return tf.estimator.EstimatorSpec(mode,loss=loss,eval_metric_ops=metrics) # [학습 모드인 경우] # 손실값을 단순히 계산하는 것 뿐만 아니라 가중치 최적화 과정이 필요 # 가중치 최적화 후, 최종적으로 함수 리턴 assert TRAIN # 학습 상태가 아닌 경우 에러 발생하도록 # adam optimizer 사용 optimizer = tf.train.AdamOptimizer(learning_rate=DEFINES.learning_rate) # 정의한 객체에 minimize 함수 실행해 최적화 짖ㄴ행 # 파라미터로 손실값 전달 train_op = optimizer.minimize(loss,global_step = tf.train.get_global_step()) return tf.estimator.EstimatorSpec(mode,loss=loss,train_op = train_op) [main] 지금까지 정의한 모듈을 실행해 실제 학습을 진행하기 위한 main 파일의 코드를 살펴보자 우선 데이터를 불러오고, 해당 데이터를 학습 데이터와 평가 데이터로 나눠 모델에 적용할 수 있도록 한다 이후 Estimator를 정의하고 모델 학습과 평가를 진행한다 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import tensorflow as tf# model 파일 불러오기import model as ml# data 파일 불러오기import dataimport numpy as npimport osimport sysfrom config import DEFINESDATA_OUT_PATH = \"./data_out/\"def main(self): # 폴더를 확인하고 생성 # DATA_OUT_PATH로 지정한 경로가 없다면 생성 # 있다면 그대로 사용 data_out_path = os.path.join(os.getcwd(),DATA_OUT_PATH) os.makedirs(data_out_path,exist_ok=True) # 앞서 정의한 load_vocabulary()와 load_data() 사용 # 단어 사전과 데이터 불러오기 word2idx, idx2word, vocabulary_length = data.load_vocabulary() # 불러온 데이터를 활용해 모델 학습 시 적용이 가능하도록 # 인코더 입력 데이터, 디코더 입력 데이터, 디코더 타깃 데이터로 만들기 train_input, train_label, eval_input, eval_label = data.load_data() train_input_enc, train_input_enc_length = data.enc_processing(train_input,word2idx) train_input_dec, train_input_dec_length = data.dec_input_processing(train_label,word2idx) train_target_dec = data.dec_target_processing(train_label,word2idx) # 평가 데이터 만들기 eval_input_enc, eval_input_enc_length = data.enc_processing(eval_input,word2idx) eval_input_dec, eval_input_dec_length = data.dec_input_processing(eval_label,word2idx) eval_target_dec = data.dec_target_processing(eval_label,word2idx) # 학습, 평가에 필요한 모든 데이터가 준비 되었으므로 # 모델 선언 후, 데이터를 적용해 학습과 평가 진행하면 OK! # 학습 과정을 저장하고 기록이 가능하도록 # 체크 포인터를 저장할 폴더 설정 # 체크 포인트는 data_out 폴더 안에 생성하도록 되어 있음 check_point_path = op.path.join(os.getcwd(),DEFINES.check_point_path) os.makedirs(check_point_path, exist_ok=True) # Estimator 객체 생성 # model_fn : 모델 함수 # model_dir : 체크 포인트를 저장할 경로 # params : 모델에 필요한 인자값들 classifier = tf.estimator.Estimator( model_fn = ml.model, model_dir = DEFINES.check_point_path, params = &#123; 'hidden_size' : DEFINES.hidden_size, 'layer_size' : DEFINES.layer_size, 'learning_rate ': DEFINES.learning_rate, 'vocabulary_length' : DEFINES.vocabulary_length, 'embedding_size' : DEFINES.embedding_size, 'embedding' : DEFINES.embedding, 'multilayer' : DEFINES.multilayer &#125;) # Estimator 이용해 학습 진행 # input_fn : 입력함수 -&gt; 앞서 정의한 학습 입력 함수 사용 # steps: 스텝 classifier.train(input_fn=lambda:data.train_input_fn( train_input_enc,train_input_dec,train_target_dec,DEFINES.batch_size), steps = DEFINES.train_steps) # 학습이 끝나면 바로 모델 평가 진행을 위해 검증 함수 정의 # input_fn : 검증 입력 함수 # 바로 모델의 성능 확인이 가능하도록 콘솔에서 정확도 출력 eval_result = classifier.evaluate(input_fn=lambda:data.eval_input_fn( eval_input_enc,eval_input_dec,eval_target_dec,DEFINES.batch_size)) print(\"\\nEVAL set accuracy: &#123;accuracy: 0.3f&#125;\\n\".format(**eval_result)) # 예시 문장을 이용해 모델의 결과가 어떤지 평가 # 예시 문장 정의하고 인코더에 적용할 수 있도록 인코더 전처리 함수에 적용 predic_input_enc, predic_input_enc_length = data.enc_processing([\"가끔 궁금해\"],word2idx) # 평가 시에는 디코더의 입력값으로 어떤 값도 들어가지 X # 빈 문자열 리스트를 넣어 디코더 입력값 전처리 predic_input_dec, predic_input_dec_length = data.dec_input_processing([\"\"],word2idx) # 학습 과정이 아니므로 디코더 출력 부분도 존재하지 X predic_target_dec = data.dec_target_processing([\"\"],word2idx) # 예측 진행 predictions = classifier.predict( input_fn=lambda:data.eval_input_fn(predic_input_enc,predic_input_dec,predic_target_dec,DEFINES.batch_size)) # 인덱스로 저장된 결과를 다시 문자열로 만들어 출력 data.pred2string(predictions,idx2word) 명령행에서 main 함수를 직접 실행할 수 있도록 하단의 코드를 추가한다 조건문을 사용해 작성된 코드가 바로 실행될 수 있게 하는데, tensorflow의 log 수준을 설정한 뒤 app.run함수를 사용해 main 함수를 실행한다 12345if __name__ == \"__main__\": tf.logging.set_verbosity(tf.logging.INFO) tf.app.run(main)tf.logging.set_verbosity [predict] 사용자에게 문장을 받아 Estimator를 통해 &lt;예측한 문장을 출력하는 역할을 하는 파일이다 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import tensorflow as tfimport dataimport sysimport model as mlfrom config import DEFINES# argc : 명령행에서 전달된 인자의 개수# argv : 실제 들어오는 인자 &lt;font size=2&gt;(문자열 값)&lt;/font&gt;if __name__ == '__main__': tf.logging.set_verbosity(tf.logging.INFO) arg_length = len(sys.argv) if(arg_length &lt; 2): raise Exception(\"Don't call us. We'll call you\") # 데이터를 통해 사전 구성 word2idx, idx2char, vocabulary_length = data.load_vocabulary() # 테스트용 데이터 생성 print(sys.argv) input = \"\" for i in sys.argv[1:]: input += i input += \" \" print(input) predic_input_enc, predic_input_enc_length = data.enc_processing([input], word2idx) # 학습 과정이 아니므로 디코딩 입력은 존재하지 X # (구조 맞추기 위함) predic_output_dec, predic_output_dec_length = data.dec_output_processing([\"\"], word2idx) # 학습 과정이 아니므로 디코딩 출력은 존재하지 X # (구조 맞추기 위함) predic_target_dec = data.dec_target_processing([\"\"], word2idx) # Estimator 구성 classifier = tf.estimator.Estimator( # 모델 등록 model_fn=ml.Model, # 체크포인트 위치 등록 model_dir=DEFINES.check_point_path, params=&#123; # 모델 쪽으로 파라메터 전달한다. # 가중치 크기 설정 'hidden_size': DEFINES.hidden_size, # 학습률 설정 'learning_rate': DEFINES.learning_rate, # 딕셔너리 크기 설정 'vocabulary_length': vocabulary_length, # 임베딩 크기 설정 'embedding_size': DEFINES.embedding_size, 'max_sequence_length': DEFINES.max_sequence_length, &#125;) for i in range(DEFINES.max_sequence_length): if i &gt; 0: predic_output_dec, predic_output_decLength = data.dec_output_processing([answer], word2idx) predic_target_dec = data.dec_target_processing([answer], word2idx) # 예측 수행 predictions = classifier.predict(input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1)) answer = data.pred_next_string(predictions, idx2char) # 예측한 값을 텍스트로 변경 print(\"answer: \", answer) 정리 및 참고 구현한 모델은 단순히 한 방향의 재현 신경망 모델을 사용했기 때문에 문장이 길어질 수록 성능이 감소할 수 있다 [참고] https://github.com/changwookjun/Transformer","categories":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/"},{"name":"NLP","slug":"BIGDATA/NLP","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/NLP/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"NLP","slug":"NLP","permalink":"https://NAEJINHJ.github.com/tags/NLP/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://NAEJINHJ.github.com/tags/Chatbot/"}]},{"title":"딥러닝 모델을 통한 챗봇 만들기 : 데이터 분석","slug":"chatbot_data_analy","date":"2019-03-02T16:09:29.346Z","updated":"2019-03-04T02:26:54.411Z","comments":true,"path":"2019/03/03/chatbot_data_analy/","link":"","permalink":"https://NAEJINHJ.github.com/2019/03/03/chatbot_data_analy/","excerpt":"","text":"[목차] 데이터 개요 데이터 분석 참고 문헌 [데이터 개요] 번역 문제에서 성능이 입증된 Sequence to sequence 모델을 활용해, 딥러닝 모델을 통한 챗봇을 만들어보자 데이터가 있어야 모델을 학습할 수 있으므로, 한글로 챗봇을 만들기 위한 데이터에 대해 먼저 알아보자 사용할 데이터는 송영숙 님의 “Chatbot_data_for_Korean v1.0” 데이터셋이다 데이터는 총 11,876개의 데이터로 구성되어 있고, 질문과 그에 대한 대답, 주제에 대한 라벨값을 가지고 있다 라벨 값은 3가지로 구성되어 있는데 0은 일상대화를, 1은 긍정을, 2는 부정의 주제를 의미한다 해당 데이터를 사용해 챗봇을 만들어보자 :) [데이터 분석] 우선 챗봇 데이터를 분석해 데이터의 고유한 특징들을 파악한 후, 모델링 과정에서 고려해야 할 사항을 확인해보자! 먼저 데이터 분석을 위해 데이터를 불러온다 123456import pandas as pdDATA_IN_PATH = \"./data_in/\"data_name = \"ChatbotData .csv\"data = pd.read_csv(DATA_IN_PATH + data_name, encoding=\"utf-8\")data.head() 각 데이터는 Q, A 값인 질문과 대답 텍스트를 가지고 있고, 그에 대한 라벨 값을 가지고 있음을 확인할 수 있다 (라벨 : 0, 1, 2) &lt;문장 전체에 대한 분석&gt; 먼저 데이터의 길이를 분석한다 질문과 답변 모두에 대한 분석을 위해 두 데이터를 하나의 리스트로 만든다 12sentences = list(data['Q']) + list(data['A']) 데이터의 길이 분석은 [세 가지 기준]으로 진행하였다 문자 단위의 길이 분석 (음절) 단어 단위의 길이 분석 (어절) 형태소 단위의 길이 분석 각 기준에 따라 토크나이징 해보자 123456789101112131415# 각 기준으로 나눈 후 길이를 측정한 값을 각각 변수로 설정해두면# 이 값을 사용해 그래프를 그리거나 각종 통곗값을 측정할 수 있다tokenized_sentences = [s.split() for s in sentences] # 띄어쓰기 기준으로 문장 나누기sent_len_by_token = [len(t) for t in tokenized_sentences]# 이 값을 다시 붙여 길이를 측정해 음절의 길이로 사용sent_len_by_eumjeol = [len(s.replace(' ','')) for s in sentences]from konlpy.tag import Oktokt = Okt()# 형태소로 나누기 위해 KoNLPy Okt 형태소 분석기 사용해 나눈 후 길이 측정morph_tokenized_sentences = [okt.morphs(s.replace(' ','')) for s in sentences]sent_len_by_morph = [len(t) for t in morph_tokenized_sentences] 우선 띄어쓰기를 기준으로 나눈 문장의 길이로 어절의 길이를 측정하고 이 값을 다시 붙인 뒤, 길이를 측정해 음절의 길이로 사용한다 형태소로 나누기 위해 Okt 형태소 분석기를 사용해 나눈 뒤 길이를 측정한다 matplotlib을 활용해 각각에 대한 그래프를 그려보자 123456789101112131415import matplotlib.pyplot as plt%matplotlib notebookplt.figure(figsize=(10,5))plt.hist(sent_len_by_token, bins=50, range=[0,50], alpha=0.5, color=\"r\",label=\"eojeol\")plt.hist(sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color=\"g\",label=\"morph\")plt.hist(sent_len_by_eumjeol, bins=50, range=[0,50], alpha=0.5, color=\"b\",label=\"eumjeol\")plt.title(\"Sentence Length Histogram\")plt.xlabel(\"Sentence Length\")plt.ylabel(\"Number of Sentences\") 어절 형태소 음절 그래프 결과를 보면, 어절이 가장 길이가 낮은 분포로 보이고, 그 다음이 형태소이며 음절 단위가 가장 긴 길이를 가지고 있는 것을 알 수 있다 다음으로 y값의 크기를 조정하여 히스토그램을 통해 각 길이가 어느 쪽으로 치우쳐 있는지, 각 데이터에 이상치는 없는지 등을 확인해보자 123456789101112131415# 데이터 길이에 대한 로그 스케일 히스토그램plt.figure(figsize=(10,5))plt.hist(sent_len_by_token, bins=50, range=[0,50], alpha=0.5, color=\"r\",label=\"eojeol\")plt.hist(sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color=\"g\",label=\"morph\")plt.hist(sent_len_by_eumjeol, bins=50, range=[0,50], alpha=0.5, color=\"b\",label=\"eumjeol\")# 추가 된 코드plt.yscale('log')plt.title(\"Sentence Length Histogram\")plt.xlabel(\"Sentence Length\")plt.ylabel(\"Number of Sentences\") 함수의 인자로 사용된 ‘log’는 각 그래프가 가지는 y 값의 스케일을 조정함으로써 차이가 큰 데이터에 대해서도 함께 비교할 수 있게 한다 y값의 스케일의 조정으로 이전에 보이지 않았던 분포의 꼬리 부분이 보기 쉽게 나온다 어절의 경우, 길이가 20인 경우가 이상치 데이터로 존재하고, 형태소나 음절의 경우, 각각 30, 50 정도의 길이에서 이상치가 존재한다 이러한 길이 분포에 대한 분석 내용을 바탕으로 입력 문장의 길이를 어떻게 설정할지 정의하면 된다 각 길이 값의 정확한 수치를 확인하기 위해 통곗값을 확인해보자 123456789import numpy as npprint('어절 최대 길이 : &#123;&#125;'.format(np.max(sent_len_by_token)))print('어절 최소 길이 : &#123;&#125;'.format(np.min(sent_len_by_token)))print('어절 평균 길이 : &#123;&#125;'.format(np.mean(sent_len_by_token)))print('어절 길이 표준편차 : &#123;&#125;'.format(np.std(sent_len_by_token)))print('어절 중간 길이 : &#123;&#125;'.format(np.median(sent_len_by_token)))print('제 1 사분위 길이 : &#123;&#125;'.format(np.percentile(sent_len_by_token,25)))print('제 3 사분위 길이 : &#123;&#125;'.format(np.percentile(sent_len_by_token,75))) 어절, 형태소, 음절 단위로 나눈 길이 값의 통곗값은 다음과 같다 전체 데이터를 한번에 보기 쉽도록 박스플롯을 그려보자 (세 가지 기준 한꺼번에 확인) 12345# 데이터 길이에 대한 박스플롯plt.figure(figsize=(10,5))plt.boxplot([sent_len_by_token,sent_len_by_morph,sent_len_by_eumjeol], labels=['Eojeol','Morph','Eumjeol'], showmeans=True) 박스 플롯을 살펴보면 꼬리가 긴 형태로 분포되어 있음을 확인할 수 있다 대체로 문장의 길이는 5 ~ 15의 길이를 중심으로 분포되어 있고 음절의 경우 길이 분포가 훨씬 더 크다는 점을 알 수 있다 지금까지 질문과 답변을 하나로 합쳐 길이 분포를 확인했다 그러나 구축할 모델의 경우, 질문이 입력으로 들어가는 부분과 답변이 입력으로 들어가는 부분이 따로 구성되어 있으므로 질문과 답변을 구분해서 분석해보자 전체 데이터가 아닌 질문과 응답으로 구성된 각 문장에 대한 길이 분포를 따로 알아보자! (형태소 기준으로만 길이 분석할 것) 123456789101112query_sentences = list(data['Q'])answer_sentences = list(data['A'])query_morph_tokenized_sentences = [okt.morphs(s.replace(' ',''))for s in query_sentences]query_sent_len_by_morph = [len(t)for t in query_morph_tokenized_sentences]answer_morph_tokenized_sentences = [okt.morphs(s.replace(' ',''))for s in answer_sentences]answer_sent_len_by_morph = [len(t)for t in answer_morph_tokenized_sentences] 우선 데이터프레임의 질문 열과 답변 열을 각각 리스트로 정의한 후, Okt 형태소 분석기를 사용해 토크나이징 한 뒤 구분된 데이터의 길이를 하나의 변수로 만든다 (질문과 답변에 대해 모두 진행) 이렇게 형태로 나눈 길이를 히스토그램으로 그려보자 12345678910# Query Length Histogram by Morph Tokenplt.figure(figsize=(10,5))plt.hist(query_sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color=\"g\",label=\"Query\")plt.hist(answer_sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color=\"r\",label=\"Answer\")plt.legend()plt.title(\"Query Length Histogram by Morph Token\")plt.xlabel(\"Query Length\")plt.ylabel(\"Number of Queries\") 히스토그램을 살펴보면 전체적으로 질문 문장 길이가 응답 문장 길이보다 상대적으로 짧다는 것을 확인할 수 있다 이상치 확인을 위해 y 값의 크기를 조정해 다시 히스토그램을 그려보자 1234567891011121314# 질문 응답 데이터 길이에 대한 로그 스케일 히스토그램plt.figure(figsize=(10,5))plt.hist(query_sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color=\"g\",label=\"Query\")plt.hist(answer_sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color=\"r\",label=\"Answer\")plt.legend()# nonposx / nonposy : [ '마스크'| '클립']# x 또는 y의 양수가 아닌 값은 유효하지 않은 것으로# 마스크되거나 매우 작은 양수로 클리핑 될 수 있음plt.yscale('log',nonposy='clip')plt.title(\"Query Length Histogram by Morph Token\")plt.xlabel(\"Query Length\")plt.ylabel(\"Number of Queries\") 질문 데이터보다 답변 데이터의 이상치 값이 더 많은 것을 확인할 수 있다 상대적으로 질문의 경우 평균 주변에 잘 분포되어 있음을 알 수 있다 두 데이터에 대해 정확한 평균값을 확인해보자 123456789101112131415print('질문 최대 길이 : &#123;&#125;'.format(np.max(query_sent_len_by_morph)))print('질문 최소 길이 : &#123;&#125;'.format(np.min(query_sent_len_by_morph)))print('질문 평균 길이 : &#123;&#125;'.format(np.mean(query_sent_len_by_morph)))print('질문 길이 표준편차 : &#123;&#125;'.format(np.std(query_sent_len_by_morph)))print('질문 중간 길이 : &#123;&#125;'.format(np.median(query_sent_len_by_morph)))print('제 1 사분위 길이 : &#123;&#125;'.format(np.percentile(query_sent_len_by_morph,25)))print('제 3 사분위 길이 : &#123;&#125;'.format(np.percentile(query_sent_len_by_morph,75)))print('답변 최대 길이 : &#123;&#125;'.format(np.max(answer_sent_len_by_morph)))print('답변 최소 길이 : &#123;&#125;'.format(np.min(answer_sent_len_by_morph)))print('답변 평균 길이 : &#123;&#125;'.format(np.mean(answer_sent_len_by_morph)))print('답변 길이 표준편차 : &#123;&#125;'.format(np.std(answer_sent_len_by_morph)))print('답변 중간 길이 : &#123;&#125;'.format(np.median(answer_sent_len_by_morph)))print('제 1 사분위 길이 : &#123;&#125;'.format(np.percentile(answer_sent_len_by_morph,25)))print('제 3 사분위 길이 : &#123;&#125;'.format(np.percentile(answer_sent_len_by_morph,75))) 통곗값을 확인해보면 최댓값의 경우 답변 데이터가 훨씬 크다는 것을 알 수 있다 평균의 경우, 질문 데이터가 좀 더 작은 값을 보인다 두 데이터를 박스플롯으로 그려보자 123plt.figure(figsize=(10,5))plt.boxplot([query_sent_len_by_morph, answer_sent_len_by_morph],labels=['Query','Answer']) 통곗값에서는 답변 데이터의 평균 길이가 질문 데이터보다 길었는데, 박스 플롯에 그려진 박스의 경우, 크기가 비슷함을 볼 수 있다 즉, 답변 데이터에 길이가 긴 이상치가 많아 평균 값이 더욱 크게 측정되었음을 알 수 있다 이 길이 값을 통해 모델에 적용될 문장의 최대 길이를 결정한다 문장 길이 3 사분위값 주변을 탐색하며 문장 생성을 가장 잘 할 수 있는 길이를 찾아본 결과, 좋은 성능이 나올 수 있는 길이는 25이다 다음으로 데이터에서 사용되는 단어에 대해 분석한다 어떤 단어가 사용되는지, 자주 사용되는 단어에는 어떤 것이 있는지 알아보자 12345678910111213141516# 각 문장에 명사, 형용사, 동사를 제외한 단어를 모두 제거한 문자열 만들기query_NVA_token_sentences = list()answer_NVA_token_sentences = list()for s in query_sentences: for token, tag in okt.pos(s.replace(' ','')): if tag == \"Noun\" or tag == \"Verb\" or tag == \"Adjective\": query_NVA_token_sentences.append(token)for s in answer_sentences: for token, tag in okt.pos(s.replace(' ','')): if tag == \"Noun\" or tag == \"Verb\" or tag == \"Adjective\": answer_NVA_token_sentences.append(token)query_NVA_token_sentences = ' '.join(query_NVA_token_sentences)answer_NVA_token_sentences = ' '.join(answer_NVA_token_sentences) 위와 같은 전처리를 수행하면 동사, 명사, 형용사를 제외한 나머지 문자는 모두 제거된 상태의 문자열이 만들어진다 이 문자열을 사용해 어휘 빈도 분석을 수행한다 워드클라우드로 질문과 답변에서 가장 많이 사용된 단어 150개를 뽑아보겠다 1234567891011import pytagcloudimport osimport matplotlib.pyplot as pltfrom collections import Countermor = okt.morphs(query_NVA_token_sentences)count = Counter(mor)tag = count.most_common(150)taglist = pytagcloud.make_tags(tag,maxsize=80)pytagcloud.create_tag_image(taglist, './wordcloud.jpg', size=(900, 600), fontname='Korean', rectangular=False) 123456mor = okt.morphs(answer_NVA_token_sentences)count = Counter(mor)tag = count.most_common(150)taglist = pytagcloud.make_tags(tag,maxsize=80)pytagcloud.create_tag_image(taglist, './answer_wordcloud.jpg', size=(900, 600), fontname='Korean', rectangular=False) 워드클라우드를 분석해보면 질문과 답변 데이터 모두 연애에 관한 단어의 빈도가 많은 것을 알 수 있다 또한 답변의 경우, 대부분 권유의 문자열을 담고 있음을 유추할 수 있다 모든 데이터 분석 과정이 끝났다 다음 포스팅에서는 분석한 결과를 토대로 데이터를 전처리하고 모델을 만들어보겠다 :) [참고 문헌] 전창욱, 최태균, 조충현 지음 위키북스","categories":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/"},{"name":"NLP","slug":"BIGDATA/NLP","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/NLP/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"NLP","slug":"NLP","permalink":"https://NAEJINHJ.github.com/tags/NLP/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://NAEJINHJ.github.com/tags/Chatbot/"}]},{"title":"Bag of Words Meets Bags of Popcorn : word2vec","slug":"imdb-word2vec","date":"2019-03-02T12:49:25.539Z","updated":"2019-03-02T15:41:35.218Z","comments":true,"path":"2019/03/02/imdb-word2vec/","link":"","permalink":"https://NAEJINHJ.github.com/2019/03/02/imdb-word2vec/","excerpt":"","text":"[목차] word2vec이란? 모델링 [word2vec이란?] 단어를 계산에 적용할 수 있는 숫자로 변환해 다음에 올 단어를 예측하는 모델이다 다음 단어를 예측하기 위한 Word Embedding 구현에는 CBOW 또는 Skip-Gram 알고리즘을 사용한다 CBOW (Continuous Bag-of-Words) CBOW 모델은 주어진 단어에 대해 앞 뒤로 C/2개 씩 총 C개의 단어를 Input으로 사용하여 주어진 단어를 맞추기 위한 네트워크를 만든다 Input Layer에서 Projection Layer로 갈 때는 모든 단어들이 공통적으로 사용하는 VxN 크기의 Projection Matrix W가 있고 (Projection Layer의 길이 N = 사용할 벡터의 길이) Projection Layer에서 Output Layer로 갈 때는 NxV 크기의 Weight Matrix W가 있다 (Projection Matrix W와 Weight Matrix W는 별개의 행렬) Input에서는 단어를 one-hot encoding으로 넣어주고, 여러 개의 단어를 각각 projection 시킨 후 해당 벡터들의 평균을 구해 Projection Layer에 내보낸다 그 뒤 Weight Matrix를 곱해 Output Layer로 내보내고 softmax 계산을 수행 한 후, 결과를 단어의 실제 one-hot encoding과 비교하여 에러를 계산한다 CBOW 모델에서 [하나의 단어를 처리하는 데에 드는 계산량]은 다음과 같다 - C개의 단어를 Projection 하는 데에 C x N - Projection Layer에서 Output Layer로 가는 데에 N x V 따라서 전체 계산량은 CxN + NxV가 된다 Skip-gram Skip-gram 모델은 CBOW와는 반대 방향 모델이라 생각할 수 있다 해당 모델은 주어진 단어 하나를 가지고 주위에 등장하는 나머지 몇 가지 단어들의 등장 여부를 유추해낸다 “가까이 위치해 있는 단어일 수록 현재 단어와 관련이 더 많은 단어일 것”이라 판단하여 멀리 떨어져있는 단어일수록 낮은 확률로 택한다 Skip-gram모델에서 [하나의 단어를 처리하는 데에 드는 계산량]은 (C개의 단어를 샘플링 했다고 할 때) - 현재 단어를 Projection하는 데에 N - Output을 계산하는 데에 NxV, 테크닉 사용 시 NxInV - 총 C개의 단어에 대해 진행해야 하므로 총 C배 따라서 총 C(N + N x InV)만큼의 연산이 필요하다 [모델링] word2vec을 활용해 모델을 구현해보자 이 경우, 단어로 표현된 리스트를 입력값으로 넣어야 하기 때문에 전처리한 넘파이 배열을 사용하지 않는다 따라서 전처리된 텍스트 데이터를 불러온 후 각 단어들의 리스트로 나눈다 12345678910111213141516171819202122import numpy as npimport pandas as pdimport osimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib notebookDATA_IN_PATH = \"./data_in/\" # 파일 저장된 경로TRAIN_CLEAN_DATA = \"train_clean.csv\"train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)reviews = list(train_data['review'])sentiments = list(train_data['sentiment'])# 전처리한 데이터의 경우 각 리뷰가 하나의 문자열로 이루어져 있음# 각 리뷰를 split 함수를 사용해 띄어쓰기를 기준으로 구분한 후# 리스트에 하나씩 추가해서 입력값 만듦sentences = []for review in reviews: sentences.append(review.split()) word2vec 벡터화 1234567# 학습 시 필요한 하이퍼 파라미터num_features = 300 # 워드 벡터 특징값 수min_word_count = 40 # 단어에 대한 최소 빈도 수num_workers = 4 # 프로세스 개수context = 10 # 컨텍스트 윈도우 크기downsampling= 1e-3 # 다운 샘플링 비율 word2vec을 학습하는 과정에서 진행 상황을 확인하기 하기 위해 logging을 사용한다 로그 수준을 INFO에 맞추면 word2vec의 학습 과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 보여준다 1234import logging#logging.basicConfig(format = '%(asctime)s : %(levelname)s :%(message)s', level=logging.INFO) word2vec을 학습하기 위해서는 word2vec 모듈에 있는 Word2Vec 객체를 생성해서 실행한다 학습하고 생성된 객체는 model 변수에 할당한다 123456789# 학습을 위한 객체의 인자는 입력할 데이터와 하이퍼 파라미터를 순서대로 입력해야 학습 가능from gensim.models import word2vecprint(\"Training model...\")model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features, min_count = min_word_count, window = context, sample = downsampling) 메시지가 나타나고 나면 학습이 완료된다 모델을 따로 저장해두면 이후에 다시 사용할 수 있기 때문에 저장해두고 이후에 학습한 값이 추가로 필요할 경우 사용하자 12model_name = \"300features_40minwords_10context\"model.save(model_name) 만들어진 word2vec 모델을 활용해 선형 회귀 모델을 학습해보자 학습을 하기 위해서 하나의 리뷰를 같은 형태의 입력값으로 만들어야 한다 현재 word2vec 모델에서 각 단어가 벡터로 표현되어 있고, 리뷰마다 단어의 개수가 모두 다르기 때문에 입력값을 하나의 형태로 만드는 것이 필요하다 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법을 사용해보자 다음은 하나의 리뷰에 대해 전체 단어의 평균값을 계산하는 함수다 123456789101112131415161718192021222324def get_features(words, model, num_features): # 출력 벡터 초기화 # 하나의 벡터를 만드는 과정에서 속도를 빠르게 하기 위함 feature_vector = np.zeros((num_features),dtype=np.float32) num_words = 0 # 어휘 사전 준비 # 문장의 단어가 해당 모델 단어 사전에 속하는지 보기 위해 # model.wv.index2word를 set 객체로 생성해 # index2word_set 변수에 할당 index2word_set = set(model.wv.index2word) # 리뷰를 구성하는 단어에 대해 임베딩된 벡터가 있는 단어 벡터의 합을 구함 for w in words: if w in index2word_set: num_words += 1 # 사전에 해당하는 단어에 대해 단어 벡터를 더함 feature_vector = np.add(feature_vector, model[w]) # 사용한 단어의 전체 개수로 나눔으로써 평균 벡터의 값 구함 # 문장의 단어 수만큼 나누어 단어 벡터의 평균 값을 문장 벡터로 함 feature_vector = np.divide(feature_vector,num_words) return feature_vector 문장에 특징값을 만들 수 있는 함수를 구현했다면 앞에서 정의한 함수를 사용해 전체 리뷰에 대해 각 리뷰의 평균 벡터를 구하는 함수를 정의한다 속도 향상을 위해 0으로 채워진 넘파이 배열을 미리 만든다 (전체 리뷰에 대한 평균 벡터를 담기 위함) 배열은 2차원으로 만드는 데 배열의 행에는 각 문장에 대한 길이를 입력하고 열에는 평균 벡터의 차원 수(크기)를 입력한다 각 리뷰에 대해 반복문을 돌면서 각 리뷰에 대해 특징값을 만든다 123456789def get_dataset(reviews, model, num_features): dataset = list() for s in reviews: dataset.append(get_features(s,model,num_features)) reviewFeatureVecs = np.stack(dataset) return reviewFeatureVecs get_dataset 함수를 구현하고 나면 실제 학습에 사용할 입력값을 만든다 12train_data_vecs = get_dataset(sentences,model,num_features) 학습과 검증 데이터셋 분리 만들어진 데이터를 가지고 학습 데이터와 검증 데이터를 나눈다 1234567891011from sklearn.model_selection import train_test_splitimport numpy as npX = train_data_vecsy = np.array(sentiments)RANDOM_SEED = 42TEST_SPLIT = 0.2X_train, X_eval, y_train, y_eval = train_test_split(X, y,test_size = TEST_SPLIT, random_state = RANDOM_SEED) 따로 뽑은 검증 데이터는 이후 학습 데이터를 통해 학습 시킨 모델의 성능을 검증할 때 사용한다 모델 선언 및 학습 모델은 로지스틱 모델을 사용한다 1234from sklearn.linear_model import LogisticRegressionlgs = LogisticRegression(class_weight=\"balanced\")lgs.fit(X_train,y_train) 각 라벨에 대해 균형있게 학습하기 위해 class_weight 인자값을 ‘balanced’로 설정한다 생성한 데이터에 학습 데이터를 적용하면 다른 데이터에 대해 학습을 진행할 수 있다 검증 데이터셋을 이용한 성능 평가 학습한 모델에 검증 데이터를 적용해 성능을 평가해보자 12print(\"Accuracy : %f\"%lgs.score(X_eval,y_eval)) # 검증 데이터로 성능 측정 Accuracy : 0.860600 데이터 제출 123456789101112131415161718192021TEST_CLEAN_DATA = \"test_clean.csv\"test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA, header=0, sep=\",\",quoting=3)test_review = list(test_data[\"review\"])test_sentences = []for review in test_data: test_sentences.append(review.split())test_data_vecs = get_dataset(test_sentences, model, num_features)DATA_OUT_PATH = \"./data_out/\"test_predicted = lgs.predict(test_data_vecs)if not os.path.exists(DATA_OUT_PATH):os.makedirs(DATA_OUT_PATH)ids = list(test_data['id'])answer_dataset= pd.DataFrame(&#123;\"id\":ids, \"sentiment\":test_predicted&#125;)answer_dataset.to_csv(DATA_OUT_PATH + \"lgs_answer.csv\")","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"Kaggle","slug":"Project/Kaggle","permalink":"https://NAEJINHJ.github.com/categories/Project/Kaggle/"},{"name":"NLP","slug":"Project/Kaggle/NLP","permalink":"https://NAEJINHJ.github.com/categories/Project/Kaggle/NLP/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"NLP","slug":"NLP","permalink":"https://NAEJINHJ.github.com/tags/NLP/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://NAEJINHJ.github.com/tags/Kaggle/"}]},{"title":"python으로 카카오톡 대화 분석하기","slug":"kakao","date":"2019-02-24T14:38:26.109Z","updated":"2019-02-25T14:50:08.264Z","comments":true,"path":"2019/02/24/kakao/","link":"","permalink":"https://NAEJINHJ.github.com/2019/02/24/kakao/","excerpt":"","text":"오늘은 카카오톡 대화를 분석해보려 한다. 분석할 데이터는 다음과 같은 방식으로 얻는다! (삼지창 메뉴 &gt; 대화 내용 &gt; 대화 내보내기) 얻은 파일의 구조는 위와 같다. 데이터 분석을 위해 먼저 파일을 불러오자! 1234path = \"./data/KakaoTalk.txt\"f = open(path,'r',encoding=\"utf-8\")data = f.read() 본격적인 분석에 들어가기 앞서, 데이터 전처리를 수행한다. 1. 시간 삭제 ex) [오후 1:40] [ 부터 시작해 ]로 끝나는 문자를 모두 삭제 123# \\\\란 해당 문자 다음에 위치한 기호를 '기호 자체'로 지칭하기 위함result = re.sub('\\\\[.+\\\\]', '', data) 2. 요일 삭제 ex) ————— 2019년 2월 18일 월요일 ————— 12result = re.sub('(--------------- 20).+(요일 ---------------)', '', result) 3. 자음 &amp; 모음 삭제 ex) ㅋㅋㅋㅋㅋ / ㅠㅠㅠㅠㅠ 123result = re.sub('[ㄱ-ㅎ]', '', result)result = re.sub('(ㅜ|ㅠ)+', '', result) 4. 기호 및 이모티콘 삭제 123result = re.sub('[~!@#$%^&amp;*()_+=?]&lt;&gt;', '', result)result = re.sub('\\\\(이모티콘\\\\)', '', result) 이제 본격적인 대화 분석에 들어가보자! 우선 분석에 필요한 라이브러리를 불러오고, 명사를 추출한다. 12345678910import osimport matplotlib.pyplot as pltfrom wordcloud import WordCloudfrom collections import Counterfrom konlpy.tag import Twitterimport pytagcloudspliter = Twitter()split = spliter.nouns(result) 추출된 명사에 포함된 의미 없는 단어를 삭제한다. 한국어는 stopwords를 직접 지정해주어야 한다. 123456stopwords = [\"나\",\"진짜\",\"난\",\"날\",\"넌\",\"내\",\"너\",\"얘\",\"쟤\",\"거\",\"그\",\"뭐\",\"개\",\"때\",\"안\", \"애\",\"그거\",\"더\",\"이\",\"것\",\"앜\",\"임\",\"엌\",\"저\",\"수\",\"또\",\"앗\",\"응\",\"걸\",\"오\", \"막\",\"알\",\"니\",\"데\",\"곳\",\"모\",\"전\",\"도\",\"머\",\"자\",\"함\",\"해\",\"번\",\"앞\",\"걔\",\"곸\",\"중\", \"줄\",\"찌\",\"구\",\"네\",\"고\",\"뎈\",\"옹\",\"아낰\",\"바\",\"후\",\"웅\",\"제\"]filtered_split= [word for word in split if word not in stopwords] stopwords를 모두 삭제했다면, 가장 많이 언급된 단어 100개를 뽑아 wordcloud 분석을 수행한다. 123456789count = Counter(filtered_split)count.most_common(50)tag = count.most_common(100)taglist = pytagcloud.make_tags(tag,maxsize=80)pytagcloud.create_tag_image(taglist, './kakao_wordcloud.jpg', size=(900, 600), fontname='Korean', rectangular=False)f.close() 해당 분석을 수행한 결과 wordcloud는 다음과 같다. (모자이크 된 부분은 이름들과 약간의 비속어임) 생각보다 언급된 비속어 빈도수가 커서 당황스러웠따… 이제부터 진짜 예쁜 말만 쓰자구욧…","categories":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/"},{"name":"NLP","slug":"BIGDATA/NLP","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/NLP/"}],"tags":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"NLP","slug":"NLP","permalink":"https://NAEJINHJ.github.com/tags/NLP/"}]},{"title":"감정 분석 Bag of Words Meets Bags of Popcorn : TF-IDF","slug":"IMDB_TF-IDF","date":"2019-02-24T12:48:25.410Z","updated":"2019-03-02T12:46:45.844Z","comments":true,"path":"2019/02/24/IMDB_TF-IDF/","link":"","permalink":"https://NAEJINHJ.github.com/2019/02/24/IMDB_TF-IDF/","excerpt":"","text":"[목차] 데이터 개요 데이터 분석 데이터 전처리 TF-IDF [데이터 개요] 워드 팝콘 워드 팝콘은 인터넷 영화 데이터베이스(IMDB)에서 나온 영화 평점 데이터를 활용한 캐글 문제다. 영화 평점 데이터이므로 각 데이터는 영화 리뷰 텍스트와 평점에 따른 감정 값(긍정 혹은 부정)으로 구성되어 있다. 영화 리뷰 데이터는 텍스트 분류에서 가장 기본적으로 사용되는 데이터로, 여기서는 이 데이터를 분류할 수 있는 모델을 학습시키려 한다. [데이터 분석] 모델을 학습시키기 전, 데이터를 전처리하는 과정을 거쳐야 한다. 우선 데이터를 불러온 후 데이터 분석 과정을 거친 뒤에 분석 결과를 바탕으로 전처리 작업을 진행할 것이다. [데이터 불러오기] 데이터를 불러올 때 pandas의 read_csv 함수를 사용 우선 데이터의 경로를 설정하고, 사용할 데이터가 탭(\\t)으로 구분되어 있으므로 delimiter 인자에 “\\t”를 설정 각 데이터에 항목명이 포함되어 있으므로 header 인자에 0 설정 쌍따옴표를 무시하기 위해 quoting 인자에 3을 설정 123456789101112import numpy as npimport pandas as pdimport osimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib notebook # 그래프를 주피터 노트북에서 바로 그리도록 함 &gt; python 3.x버전부턴 이거DATA_IN_PATH = \"./data_in/\"train_data = pd.read_csv(DATA_IN_PATH + \"labeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)train_data.head() 데이터는 “id”, “sentiment”, “review”로 구분되어 있으며, 각 리뷰(review)에 대한 감정(sentiment)이 긍정(1) / 부정(0)인지 나와있다 [데이터 분석] 1. 데이터 크기 2. 데이터의 개수 3. 각 리뷰의 문자 길이 분포 4. 많이 사용된 단어 5. 긍정, 부정 데이터의 분포 6. 각 리뷰의 단어 개수 분포 7. 특수문자 및 대문자, 소문자 비율 1. 데이터의 크기 tsv 파일 중에서 zip 파일이 아닌 파일들을 가져와 크기를 출력한다. 123456print(\"파일 크기: \")for file in os.listdir(DATA_IN_PATH): if 'tsv' in file and 'zip' not in file: print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH+file)/1000000, 2)) + 'MB') 결과를 확인하면 unlabeledTrainData.tsv 파일이 가장 크고 나머지 두 파일의 크기는 비슷하다는 것을 알 수 있다. 즉, 라벨이 있는 학습 데이터와 평가 데이터의 크기는 비슷하고, 라벨이 없는 학습 데이터의 크기가 가장 크다 2. 데이터의 개수 12print('전체 학습 데이터의 개수: &#123;&#125;'.format(len(train_data))) 3. 각 리뷰의 문자 길이 분포 각 데이터의 문자의 길이를 알아보자 ‘review’ 열에 각 데이터의 리뷰가 들어가 있다 각 리뷰의 길이를 새로운 변수로 정의한다. 123train_length = train_data['review'].apply(len)train_length.head() 각 리뷰의 길이가 담긴 변수를 사용해 히스토그램을 그려보자 (matplotlib을 사용) 12345678910111213141516171819202122# 그래프에 대한 이미지 크기 선언# figsize: (가로, 세로) 형태의 튜플로 입력plt.figure(figsize=(12,5)) # 그릴 그래프의 크기 설정# 히스토그램 선언# bins: 히스토그램 값에 대한 버킷 범위# range: x축 값의 범위# alpha: 그래프 색상 투명도# color: 그래프 색상# label: 그래프에 대한 라벨plt.hist(train_length,bins=200,alpha=0.5,color=\"r\",label=\"word\")plt.yscale('log',nonposy='clip') # y 값이 log의 크기를 가지도록 설정# 그래프 제목plt.title('Log-Histogram of length of review')# 그래프 x 축 라벨plt.xlabel('Length of review')# 그래프 y 축 라벨plt.ylabel('Number of review') 분포를 보면 각 리뷰의 문자 길이가 대부분 6000이하이고 대부분 2000 이하에 분포되어 있음을 알 수 있음 일부 데이터의 경우 이상치로 10000 이상의 값을 가지고 있음 길이에 대해 몇가지 통곗값을 확인해보자. 1234567891011# 길이에 대해 몇 가지 통곗값 확인print('리뷰 길이 최댓값: &#123;&#125;'.format(np.max(train_length)))print('리뷰 길이 최솟값: &#123;&#125;'.format(np.min(train_length)))print('리뷰 길이 평균값: &#123;&#125;'.format(np.mean(train_length)))print('리뷰 길이 표준편차: &#123;&#125;'.format(np.std(train_length)))print('리뷰 길이 중간값: &#123;&#125;'.format(np.median(train_length)))# 사분위의 대한 경우는 0~100 스케일로 되어 있음print('리뷰 길이 제1사분위: &#123;&#125;'.format(np.percentile(train_length,25)))print('리뷰 길이 제3사분위: &#123;&#125;'.format(np.percentile(train_length,75))) 평균이 약 1300이고. 최댓값이 13000이라는 것을 알 수 있다 이 값을 가지고 박스플롯을 그려보면, 12345678plt.figure(figsize=(12,5))# 박스 플롯 생성# 첫 번째 인자 : 여러 분포에 대한 데이터 리스트를 입력# labels : 입력한 데이터에 대한 라벨# showmeans: 평균값을 마크함plt.boxplot(train_length,labels=['counts'],showmeans=True) 박스플롯 그래프를 통해 데이터를 살펴보면 우선 데이터의 길이가 대부분 2000 이하로 평균이 1500 이하인데, 길이가 4000 이상인 이상치 데이터도 많이 분포되어 있음을 확인 가능 4. 많이 사용된 단어 리뷰에 많이 사용된 단어로 어떤 것이 있는지 알아보자 워드 클라우드 라이브러리를 사용한다 123456from wordcloud import WordCloudcloud = WordCloud(width=800,height=600).generate(\" \".join(train_data['review']))plt.figure(figsize=(20,15))plt.imshow(cloud)plt.axis('off') 워드 클라우드를 통해 그린 그림을 살펴보면 데이터에서 가장 많이 사용된 단어가 br임을 확인할 수 있다. HTML 태그 중 하나로 해당 데이터가 정제되지 않은 인터넷 상의 리뷰 형태로 작성 되어 있음을 확인 가능하다 이후 전처리 작업에서 이 태그를 제거하는 과정이 필요하다 5. 긍정, 부정 데이터의 분포 각 라벨의 분포를 확인해보자 해당 데이터의 경우 긍정과 부정이라는 두 가지 라벨만 가지고 있다 분포의 경우 seaborn을 확인해 시각화 한다 1234fig, axe = plt.subplots(ncols=1)fig.set_size_inches(6,3)sns.countplot(train_data['sentiment']) 라벨의 분포 그래프를 보면 거의 동일한 개수로 분포되어 있음을 확인할 수 있다 각 라벨에 대해 정확한 값을 확인해보자 123print(\"긍정 리뷰 개수: &#123;&#125;\".format(train_data['sentiment'].value_counts()[1]))print(\"부정 리뷰 개수: &#123;&#125;\".format(train_data['sentiment'].value_counts()[0])) 결과를 보면 정확하게 같은 값을 가진다는 것을 확인할 수 있다 6. 각 리뷰의 단어 개수 분포 각 리뷰를 단어 기준으로 나눠서 각 리뷰당 단어의 개수를 확인해보자 단어는 띄어쓰기 기준으로 하나의 단어라 생각하고 개수를 계산한다 우선 각 단어의 길이를 가지는 변수를 하나 설정한다 123456789101112131415# 각 단어의 길이를 가지는 변수 선언train_word_counts = train_data['review'].apply(lambda x:len(x.split(' ')))# 히스토그램plt.figure(figsize=(15,10))plt.hist(train_word_counts,bins=50,alpha=0.5,color=\"purple\",label=\"train\")# 그래프 제목plt.title('Log-Histogram of word count of review')plt.yscale('log',nonposy='clip') # y 값이 log의 크기를 가지도록 설정plt.legend()# 그래프 x 축 라벨plt.xlabel('Number of words',fontsize=15)# 그래프 y 축 라벨plt.ylabel('Number of reviews',fontsize=15) 히스토그램을 그려보면 대부분의 단어가 1000개 미만의 단어를 가지고 있고, 대부분 200개 정도의 단어를 가지고 있음을 확인할 수 있다. 몇 가지 통곗값을 확인해보면, 12345678910print('리뷰 단어 개수 길이 최댓값: &#123;&#125;'.format(np.max(train_word_counts)))print('리뷰 단어 개수 길이 최솟값: &#123;&#125;'.format(np.min(train_word_counts)))print('리뷰 단어 개수 길이 평균값: &#123;:.2f&#125;'.format(np.mean(train_word_counts)))print('리뷰 단어 개수 길이 표준편차: &#123;:.2f&#125;'.format(np.std(train_word_counts)))print('리뷰 단어 개수 길이 중간값: &#123;&#125;'.format(np.median(train_word_counts)))# 사분위의 대한 경우는 0~100 스케일로 되어 있음print('리뷰 단어 개수 길이 제1사분위: &#123;&#125;'.format(np.percentile(train_word_counts,25)))print('리뷰 단어 개수 길이 제3사분위: &#123;&#125;'.format(np.percentile(train_word_counts,75))) 단어 개수의 경우 평균이 233개이고, 최댓값의 경우 2,470개의 단어를 가지고 있다. 3사분위 값이 284개로 75%가 300개 이하의 단어를 가지고 있음을 확인할 수 있다 7. 특수문자 및 대문자, 소문자 비율 각 리뷰에 대해 구두점과 대소문자 비율 값을 확인해보자 12345678910111213# 물음표가 구두점으로 쓰임qmarks = np.mean(train_data['review'].apply(lambda x: '?' in x))fullstop = np.mean(train_data['review'].apply(lambda x: '.' in x)) # 마침표capital_first = np.mean(train_data['review'].apply(lambda x: x[0].isupper())) # 첫번째 대문자capitals = np.mean(train_data['review'].apply(lambda x:max([y.isupper() for y in x]))) # 대문자 개수numbers = np.mean(train_data['review'].apply(lambda x:max([y.isdigit() for y in x])))print('물음표가 있는 질문: &#123;:.2f&#125;%'.format(qmarks*100))print('마침표가 있는 질문: &#123;:.2f&#125;%'.format(fullstop*100))print('첫 글자가 대문자인 질문: &#123;:.2f&#125;%'.format(capital_first*100))print('대문자가 있는 질문: &#123;:.2f&#125;%'.format(capitals*100))print('숫자가 있는 질문: &#123;:.2f&#125;%'.format(numbers*100)) 결과를 보면 대부분 마침표를 포함하고 있고, 대문자도 대부분 사용하고 있다. 따라서 전처리 과정에서 대문자의 경우 모두 소문자로 바꾸고 특수 문자의 경우 제거한다 이 과정은 학습에 방해가 되는 요소를 제거하기 위함이다 [데이터 전처리] 데이터를 모델에 적용할 수 있도록 데이터 전처리를 진행한다 데이터 전처리 과정에서 사용할 라이브러리는 다음과 같다 12345678910import re # BeautifulSoup과 함께 데이터 정제를 위함import pandasimport numpy # 전처리 된 데이터 저장을 위함import jsonfrom bs4 import BeautifulSoupfrom nltk.corpus import stopwords# 텐서플로우의 전처리 모듈from tensorflow.python.keras.preprocessing.sequence import pad_sequencesfrom tensorflow.python.keras.preprocessing.text import Tokenizer 본격적으로 전처리 과정을 진행하는데, 어떤 방향으로 전처리해야 할 지 결정하기 위해 학습 데이터를 불러온 후, 첫 번째 학습 데이터의 리뷰를 출력해본다 1234DATA_IN_PATH = \"./data_in/\"train_data = pd.read_csv(DATA_IN_PATH + \"labeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)print(train_data['review'][0]) # 첫번째 리뷰 데이터 리뷰 데이터를 보면 문장 사이에 HTML 태그와 ‘\\’, ‘∙∙∙’ 같은 특수 문자가 포함된 것을 확인할 수 있다 문장부호 및 특수 문자는 일반적으로 문장의 의미에 크게 영향을 미치지 않기 때문에 최적화된 학습을 위해 제거하는 것이 좋다 모든 전처리 과정을 하나의 함수로 정의하면 다음과 같다 12345678910111213141516171819202122232425262728def preprocessing(review,remove_stopwords = False): # 불용어 제거는 옵션으로 선택 가능 # 1. HTML 태그 제거 review_text = BeautifulSoup(review,\"html5lib\").get_text() # HTML 태그 제거 # 영어가 아닌 특수문자를 공백으로 바꾸기 review_text = re.sub(\"[^a-zA-Z ]\",\"\",review_text) # 대문자를 소문자로 바꾸고 공백 단위로 텍스트를 나눠서 리스트로 만들기 words = review_text.lower().split() if remove_stopwords: # 불용어 제거 # 영어 불용어 불러오기 stop_words = set(stopwords.words('english')) # 불용어가 아닌 단어로 이뤄진 새로운 리스트 생성 words = [w for w in words if not w in stop_words] # 단어 리스트를 공백을 넣어 하나의 글로 합침 clean_review = ' '.join(words) # 불용어를 제거하지 않을 때 else: clean_review = ' '.join(words) return clean_review 불용어 제거는 인자값으로 받아서 선택할 수 있도록 했다 전체 데이터에 대해 전처리를 진행한 후 데이터를 하나 확인해보자 12345678clean_train_reviews = []for review in train_data['review']: clean_train_reviews.append(preprocessing(review,remove_stopwords=True))# 전처리한 데이터의 첫 번째 데이터 출력clean_train_reviews[0] 모델에 따라 각 리뷰가 단어들의 인덱스로 구성된 벡터가 아닌 텍스트로 구성되어야 하는 경우도 있으므로 지금까지 전처리한 데이터를 데이터프레임으로 만들어두고 이후 전처리 과정이 모두 끝난 후 전처리한 데이터를 저장할 때 함께 저장하게 한다 123clean_train_df = pd.DataFrame(&#123;'review':clean_train_reviews,'sentiment':train_data['sentiment']&#125;) [두 가지 전처리 과정]이 남았다 - 전처리한 데이터에서 각 단어를 인덱스로 벡터화 - 패딩 과정 단어들을 벡터화하고 패딩하는 과정을 그림을 통해 직관적으로 살펴보자 우선 원본 텍스트 데이터를 인덱스 벡터로 변환해야 한다 이러한 변환을 위해 인덱싱 단어 사전을 생성해서 활용한다 그 후 고정된 길이에 대해 패딩 처리를 한다 각 단어 인덱스로 벡터화 Tokenizer 모듈을 생성한 후 정제된 데이터에 적용하고 인덱스로 구성된 벡터로 변환한다 123456# 각 리뷰가 텍스트가 아닌 인덱스의 벡터로 구성될 것tokenizer = Tokenizer()tokenizer.fit_on_texts(clean_train_reviews)text_sequence = tokenizer.texts_to_sequences(clean_train_reviews)print(text_sequence[0]) 결과를 보면 텍스트로 되어 있던 첫 번째 리뷰가 각 단어의 인덱스로 바뀐 것을 볼 수 있다 각 인덱스가 어떤 단어를 의미하는지 확인할 수 있어야 하기 때문에 단어 사전이 필요하다 1234# 단어 사전# 각 인덱스가 어떤 단어를 의미하는 지 확인 위함word_vocab = tokenizer.word_indexprint(word_vocab) 전체 데이터에서 사용한 단어 개수가 총 몇 개인지 확인해보면 12print(\"전체 단어 개수: \",len(word_vocab)) 전체 단어 개수: 137872 단어 사전 뿐만 아니라 전체 단어 개수도 이후 모델에서 사용되기 때문에 저장해둔다 데이터에 대한 정보인 단어 사전과 전체 단어 개수는 새롭게 딕셔너리 값을 지정해서 저장해둔다 1234data_configs = &#123;&#125;data_configs['vocab'] = word_vocabdata_configs['vocab_size'] = len(word_vocab) + 1 패딩 과정 모델에 따라 입력값의 길이가 동일해야 하기 때문에 일정 길이로 자르고 부족한 부분은 특정값으로 채우는 패딩 과정이 필요하다 따라서 특정 길이를 최대 길이로 설정하고 더 긴 데이터의 경우 뒷부분을 자르고 짧은 데이터의 경우에는 0 값으로 패딩하는 작업을 진행 12345678MAX_SEQUENCE_LENGTH = 174 # 문장 최대 길이 &gt;&gt; 단어 개수 통계의 중간값# 일부 이상치 데이터의 길이가 지나치게 길면 평균이 급격히 올라갈 수 있기 때문에# 적당한 값인 중간값을 사용하는 것# pad_sequences(패딩을 적용할 데이터, 최대 길이값, 0 값을 데이터 앞에 넣을 지 뒤에 넣을지 여부 설정)train_inputs = pad_sequences(text_sequence,maxlen=MAX_SEQUENCE_LENGTH,padding=\"post\")print('Shape of train data: ',train_inputs.shape) Shape of train data: (25000, 174) 패딩 처리를 통해 데이터의 형태가 25,000개의 데이터가 174라는 길이를 동일하게 가지게 되었음을 확인할 수 있다 전처리 데이터 저장 마지막으로 학습 시 라벨(정답을 나타내는 값)을 넘파이 배열로 저장한다 이후 전처리한 데이터를 저장할 때 넘파이 형태로 저장하기 때문이다 12train_labels = np.array(train_data['sentiment'])print(\"Shape of label tensor: \",train_labels.shape) Shape of label tensor: (25000,) 넘파이 배열로 만든 후 라벨의 형태를 확인해보면 길이가 25,000인 벡터임을 확인할 수 있다 (데이터 하나당 하나의 값을 가지는 형태) 라벨까지 넘파이 배열로 저장하면 모든 전처리 과정이 끝난다 전처리한 데이터를 이후 모델링 과정에 사용하기 위해 저장한다 (총 4개의 데이터) 정제된 텍스트 데이터 벡터화한 데이터 정답 라벨 데이터 정보(단어 사전, 전체 단어 개수) 텍스트 데이터 : CSV 파일 벡터화한 데이터와 정답 라벨 : numpy 파일 데이터 정보 : JSON 파일 (딕셔너리 형태이기 때문) 1234567891011121314151617181920DATA_IN_PATH = \"./data_in/\"TRAIN_INPUT_DATA = \"train_input.npy\"TRAIN_LABEL_DATA = \"train_label.npy\"TRAIN_CLEAN_DATA = \"train_clean.csv\"DATA_CONFIGS = \"data_configs.json\"import os# 저장하는 디렉터리가 존재하지 않으면 생성if not os.path.exists(DATA_IN_PATH): os.makedirs(DATA_IN_PATH)# 전처리된 데이터를 넘파이 형태로 저장np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA,\"wb\"),train_inputs)np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA,\"wb\"),train_labels)# 정제된 텍스트를 CSV 형태로 저장clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA,index=False)# 데이터 사전을 JSON 형태로 저장json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS,\"w\"),ensure_ascii=False) 평가 데이터 전처리 평가 데이터 전처리 역시 학습 데이터와 동일한 과정이나 평가 데이터의 경우 라벨 값이 없기 때문에 라벨은 따로 저장하지 않아도 되고 데이터 정보인 단어 사전과 단어 개수에 대한 정보도 학습 데이터의 것을 사용하므로 저장하지 않아도 된다 추가로 각 리뷰 데이터에 대해 리뷰에 대한 “id” 값을 저장해야 한다 나머지 부분은 학습 데이터와 동일하게 전처리를 진행한다 1234567891011121314test_data = pd.read_csv(DATA_IN_PATH + \"testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)clean_test_reviews = []for review in test_data['review']: clean_test_reviews.append(preprocessing(review,remove_stopwords=True))clean_test_df = pd.DataFrame(&#123;'review':clean_train_reviews,'id':test_data['id']&#125;)test_id = np.array(test_data['id'])tokenizer.fit_on_texts(clean_test_reviews)text_sequence = tokenizer.texts_to_sequences(clean_test_reviews)test_inputs = pad_sequences(text_sequence,maxlen=MAX_SEQUENCE_LENGTH,padding=\"post\") 평가 데이터를 전처리할 때 토크나이저를 통해 인덱스 벡터로 만들 때 토크나이징 객체로 새롭게 만드는 것이 아니라, 기존의 학습 데이터에 적용한 토크나이저 객체를 사용 (새롭게 만들 경우 학습 데이터와 평가 데이터에 대한 각 단어들의 인덱스가 달라지기 때문 &gt; 모델에 정상적으로 적용 불가) 평가 데이터를 전처리한 데이터도 위와 동일하게 저장한다 123456789# 평가 데이터를 전처리한 데이터도 위와 동일하게 저장TEST_INPUT_DATA = \"test_input.npy\"TEST_CLEAN_DATA = \"tesT_clean.csv\"TEST_ID_DATA = \"test_id.npy\"np.save(open(DATA_IN_PATH + TEST_INPUT_DATA,\"wb\"),test_inputs)np.save(open(DATA_IN_PATH + TEST_ID_DATA,\"wb\"),test_id)# 정제된 텍스트를 CSV 형태로 저장clean_train_df.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA,index=False) [TF-IDF] TfidVectorizer는 TF-IDF라는 특정한 값을 사용해서 텍스트 데이터의 특징을 추출하는 방법이다 TF-IDF란 TF와 IDF 두 값을 곱해서 사용하므로 어떤 단어가 해당 문서에 자주 등장하지만 다른 문서에는 많이 없는 단어일수록 높은 값을 가지게 된다 따라서 조사나 지시대명사처럼 자주 등장하는 단어는 TF 값은 크지만 IDF 값은 작다 (CountVectorizer가 가진 문제점 해결 가능) TfidVectorizer를 사용하기 위해서는 입력값이 텍스트로 이루어진 데이터 형태여야 한다 따라서 전처리한 결과 중 넘파이 배열이 아닌 정제된 텍스트 데이터를 사용한다 우선 데이터를 불러온다 12345678DATA_IN_PATH = \"./data_in/\"TRAIN_CLEAN_DATA = \"train_clean.csv\"train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, header=0,delimiter=\"\\t\",quoting=3)reviews = list(train_data['review'])sentiments = list(train_data['sentiment']) 판다스를 이용해 전처리한 텍스트 데이터를 불러온 후 리뷰값과 라벨값을 각각 따로 리스트로 지정해 둔다 1. TF-IDF 벡터화 데이터에 대해 TF-IDF 값으로 벡터화를 진행한다 객체를 생성할 때 몇 가지 인자값을 설정하는데 하나씩 살펴보자 12345from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=5000)X = vectorizer.fit_transform(reviews) # 전체 문장에 대한 특징 벡터 데이터 X TfidVectorizer를 생성한 후 fit_transform 함수를 사용해서 전체 문장에 대한 특징 벡터 데이터 X를 생성한다 해당 데이터를 학습 데이터와 검증 데이터로 분리해보자 2. 학습과 검증 데이터셋 분리 1234567891011from sklearn.model_selection import train_test_splitimport numpy as npRANDOM_SEED = 42TEST_SPLIT = 0.2y = np.array(sentiments)X_train, X_eval, y_train, y_eval = train_test_split(X,y,test_size = TEST_SPLIT, random_state=RANDOM_SEED) 입력값인 X와 정답 라벨을 넘파이 배열로 만든 y에 대해 적용해 학습 데이터와 검증 데이터로 나눴다 검증데이터는 기존 학습 데이터의 20%로 설정해 만들었다 3. 모델 선언 및 학습 선형 회귀 모델을 만들기 위해 LogisticRegression 클래스의 객체를 생성 이후 해당 객체의 fit 함수를 호출하면 모델 학습이 진행된다 class_weight를 ‘balanced’로 설정하면 각 라벨에 대해 균형 있게 학습이 가능하다 12345from sklearn.linear_model import LogisticRegressionlgs = LogisticRegression(class_weight='balanced') # 각 라벨에 대해 균형 있게 학습 가능lgs.fit(X_train,y_train) 정의한 모델에 검증 데이터를 사용해 성능을 측정해보자 4. 검증 데이터로 성능 평가 12print(\"Accuracy : &#123;&#125;\".format(lgs.score(X_eval,y_eval))) Accuracy : 0.8576 5. 데이터 제출 전처리한 텍스트 형태의 평가 데이터를 불러온다 학습 데이터와 마찬가지로 파일명 설정 후, 데이터 프레임 형태로 불러온다 12345TEST_CLEAN_DATA = \"tesT_clean.csv\"test_data = pd.read_csv(DATA_IN_PATH+TEST_CLEAN_DATA, header=0, delimiter=\"\\t\",quoting=3) 불러온 데이터를 대상으로 이전에 사용했던 객체를 사용해 TF-IDF 값으로 벡터화 한다 벡터화할 때 평가 데이터에 대해서는 fit을 호출하지 않고 transform만 호출한다 12testDataVecs = vectorizer.transform(test_data['review']) 이 값으로 예측한 후, 예측값을 하나의 변수로 할당하고 출력해서 형태를 확인해본다 123test_predicted = lgs.predict(testDataVecs)print(test_predicted) [0 1 0 … 0 0 1] 결과를 보면 각 데이터에 대해 긍정, 부정 값을 가지고 있다 값을 캐글에 제출하기 위해 데이터프레임 형태로 만들어 CSV 파일로 저장하자 캐글 제출을 위한 데이터는 고유한 id 값과 결괏값으로 구성되어야 한다 123456789DATA_OUT_PATH = \"./data_out/\"if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)ids = list(test_data['id'])answer_dataset = pd.DataFrame(&#123;\"id\": ids,\"sentiment\":test_predicted&#125;)answer_dataset.to_csv(DATA_OUT_PATH + \"lgs_tfidf_answer.csv\")","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"Kaggle","slug":"Project/Kaggle","permalink":"https://NAEJINHJ.github.com/categories/Project/Kaggle/"},{"name":"NLP","slug":"Project/Kaggle/NLP","permalink":"https://NAEJINHJ.github.com/categories/Project/Kaggle/NLP/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"NLP","slug":"NLP","permalink":"https://NAEJINHJ.github.com/tags/NLP/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://NAEJINHJ.github.com/tags/Kaggle/"}]},{"title":"뉴스 댓글 WordCloud 분석","slug":"wordcloud","date":"2019-02-20T14:22:24.864Z","updated":"2019-02-21T03:32:49.177Z","comments":true,"path":"2019/02/20/wordcloud/","link":"","permalink":"https://NAEJINHJ.github.com/2019/02/20/wordcloud/","excerpt":"","text":"[목적] 오늘은 뉴스의 댓글을 분석해, 해당 뉴스에서 가장 많이 언급 된 단어를 알 수 있는 wordcloud 분석을 수행해보겠다. 이 분석으로 우리는 해당 기사의 여론을 알 수 있다. 분석할 뉴스는 아래와 같다. (링크: https://news.naver.com/main/ranking/read.nhn?rankingType=popular_memo&amp;oid=056&amp;aid=0010672808&amp;date=20190220&amp;type=2&amp;rankingSectionId=102&amp;rankingSeq=2) 댓글 분석을 위해 먼저, 총 9,809개의 댓글을 크롤링한다 [크롤링] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 라이브러리from bs4 import BeautifulSoupimport requestsimport reimport sysimport pprint# 댓글 저장 리스트 초기화List=[]# 네이버 뉴스 url을 입력url=\"https://news.naver.com/main/ranking/read.nhn?rankingType=popular_memo&amp;oid=056&amp;aid=0010672808&amp;date=20190220&amp;type=2&amp;rankingSectionId=102&amp;rankingSeq=2\"oid=url.split(\"oid=\")[1].split(\"&amp;\")[0]aid=url.split(\"aid=\")[1]page=1header = &#123; \"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\", \"referer\":url,&#125;while True : c_url=\"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&amp;templateId=default_society&amp;pool=cbox5&amp;_callback=jQuery1707138182064460843_1523512042464&amp;lang=ko&amp;country=&amp;objectId=news\"+oid+\"%2C\"+aid+\"&amp;categoryId=&amp;pageSize=20&amp;indexSize=10&amp;groupId=&amp;listType=OBJECT&amp;pageType=more&amp;page=\"+str(page)+\"&amp;refresh=false&amp;sort=FAVORITE\"# 파싱 r=requests.get(c_url,headers=header) cont=BeautifulSoup(r.content,\"html.parser\") total_comm=str(cont).split('comment\":')[1].split(\",\")[0] match=re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(cont))# 댓글을 리스트에 중첩 List.append(match)# 한번에 댓글이 20개씩 보이기 때문에 한 페이지씩 댓글 크롤링.# 페이지 내, 댓글의 개수가 20개보다 적을 시 마지막 페이지임을 식별 if int(total_comm) &lt;= ((page) * 20): break else : page+=1# 여러 리스트들을 하나로 묶는 함수def flatten(l): flatList = [] for elem in l: # if an element of a list is a list # iterate over this list and add elements to flatList if type(elem) == list: for e in elem: flatList.append(e) else: flatList.append(elem) return flatList# 리스트 결과 확인# flatten(List)import codecsfile = codecs.open(\"./comment/news_test_set.txt\",'w',encoding='utf8')# file = codecs.open(\"./comment/news_test_set.txt\",'a',encoding='utf8')file.write(\"\\n\".join(str(v) for v in List))file.close() 크롤링된 댓글은 다음과 같이 txt 파일로 저장된다. 크롤링된 댓글에서 명사를 추출하고, 어떤 단어가 가장 많이 나왔는 지 빈도수를 체크한다. 언급 된 빈도가 큰 150가지의 단어들로 wordcloud를 만든다. pytagcloud의 경우, 한글을 지원하지 않으므로 한글 폰트(ttf,otf 파일)를 다운받아 pytagcloud\\fonts 폴더 내에 저장하고, 해당 경로 내, fonts.json 파일에 아래 처럼 코드를 추가한다. 123456789101112[ &#123; \"name\": \"Korean\", \"ttf\": \"본인이 다운 받은 한글 파일\", \"web\": \"http://fonts.googleapis.com/css?family=Nobile\" &#125;, &#123; \"name\": \"Nobile\", \"ttf\": \"nobile.ttf\", \"web\": \"http://fonts.googleapis.com/css?family=Nobile\" &#125;, [WordCloud] 12345678910111213141516171819202122import osimport matplotlib.pyplot as pltfrom collections import Counter# KoNLPy v0.4.5. 상위 버전부터 Twitter가 Okt로 대체# twitter로 쓴다면 이하의 에러 발생# \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.from konlpy.tag import Oktimport pytagcloudf = open(\"./comment/news_test_set.txt\",'r',encoding=\"UTF8\")data = f.read()spliter = Okt()nouns = spliter.nouns(data)count = Counter(nouns)tag = count.most_common(150)taglist = pytagcloud.make_tags(tag,maxsize=80)pytagcloud.create_tag_image(taglist, './wordcloud/news_wordcloud.jpg', size=(900, 600), fontname='Korean', rectangular=False)f.close() 가장 많이 언급된 단어들을 나열해보자. wordcloud 분석의 결과는 다음과 같다. [결론] 해당 분석을 통해 뉴스에 대한 여론이 꽤나 부정적임을 알 수 있었다. [출처] 네이버 뉴스 댓글 크롤링 https://m.blog.naver.com/PostView.nhn?blogId=seodaeho91&amp;logNo=221273565367&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F 텍스트마이닝 : 파이썬 워드클라우드 만들기 http://blog.naver.com/PostView.nhn?blogId=imsam77&amp;logNo=221260319159&amp;categoryNo=13&amp;parentCategoryNo=0&amp;viewDate=&amp;currentPage=1&amp;postListTopCurrentPage=1&amp;from=postView","categories":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/"},{"name":"NLP","slug":"BIGDATA/NLP","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/NLP/"}],"tags":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"NLP","slug":"NLP","permalink":"https://NAEJINHJ.github.com/tags/NLP/"}]},{"title":"참고할 만한 데이터셋","slug":"DATA","date":"2019-01-27T17:15:26.285Z","updated":"2019-01-27T17:36:31.443Z","comments":true,"path":"2019/01/28/DATA/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/28/DATA/","excerpt":"","text":"DATA MLB Game Logs 1871-2016 https://data.world/dataquest/mlb-game-logs Football https://datahub.io/search?q=football","categories":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/"},{"name":"DATA","slug":"BIGDATA/DATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/DATA/"}],"tags":[{"name":"data","slug":"data","permalink":"https://NAEJINHJ.github.com/tags/data/"},{"name":"modeling","slug":"modeling","permalink":"https://NAEJINHJ.github.com/tags/modeling/"}]},{"title":"PLAN","slug":"upload-plan","date":"2019-01-27T16:40:47.333Z","updated":"2019-03-02T16:06:09.659Z","comments":true,"path":"2019/01/28/upload-plan/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/28/upload-plan/","excerpt":"","text":"UPLOAD 메이저 리그 야구 승률 예측 - 음이향 모형 사용한 시뮬레이터 AirKorea 데이터 분석 코딩테스트 뉴스 클러스터링 웹 크롤링 (두산베어스 온라인 스토어) kaggle 세일 예측 - Predict Future Sales PLAN 규동 메뉴 이미지 판정 챗봇(회화봇) OpenCV로 얼굴 인식 &amp; 모자이크 프로그램","categories":[{"name":"LOG","slug":"LOG","permalink":"https://NAEJINHJ.github.com/categories/LOG/"}],"tags":[{"name":"PLAN","slug":"PLAN","permalink":"https://NAEJINHJ.github.com/tags/PLAN/"},{"name":"LOG","slug":"LOG","permalink":"https://NAEJINHJ.github.com/tags/LOG/"}]},{"title":"메이저리그 야구 승률 예측 - ① 게임-데이 시뮬레이션","slug":"baseball-winner-prediction","date":"2019-01-26T14:55:11.387Z","updated":"2019-01-27T16:37:05.172Z","comments":true,"path":"2019/01/26/baseball-winner-prediction/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/26/baseball-winner-prediction/","excerpt":"","text":"[목차] 개요 데이터 분석 게임-데이 시뮬레이션 (승리하는 팀을 가려내기 위한 예측 모델링 프레임 워크) [개요] 일반에 공개된 메이저 리그 야구 데이터를 사용하여 타우트 없이 승리팀을 가려낼 수 있는 데이터 기반 분석 (타우트 - 경마나 스포츠 경기에서 도박 정보나 팁을 파는 사람) ▶ 설명 변수 과거 선수의 역량(타율, 방어율 등)과 팀의 승률과 관련 이러한 변수에 근거해 상대팀의 득점 예측 이외에도 게임이 진행될 장소와 시기, 게임 결과에 영향을 줄 수 있는 다른 모든 조건 고려 ▷ 반응 변수 승 / 패/ 득점 수 (득점 스코어 - 게임, 팀, 시간 or 일정으로 구성) ▶ 분석 방법 일반적인 분석과 동일하게, 훈련-테스트 방법 이용해 모형 평가 ▷ 승점 모형 전통적 방법, 데이터 적응(data adaptive) 방법 두 가지 방법의 조합 데이터 분석 미래 득점 예측 위해 과거 승리 기록 참조 2007년 시즌 분석 위해 메츠와 양키스 간 해당 시즌에 대한 데이터 선택 (이전 시즌에 얻은 팀 통계와 2007년 프리시즌 기록은 무시) [뉴욕 메츠의 경기와 득점에 대한 모든 정보](4/1~6/15) 홈/ 원정 경기에서 모두 상대팀보다 많은 점수 기록 [뉴욕 양키스의 경기와 득점에 대한 모든 정보](4/1~6/15) 원정 경기에서 상대팀보다 많은 점수 기록 but 홈 경기에서 적은 점수 기록 5월 18일 양키스 2: 메츠 3 5월 19일 양키스 7: 메츠 10 5월 20일 양키스 6: 메츠 2 이와 같은 세 경기의 결과를 사용하여 6월 15~17일의 게임 결과를 예측 게임-데이 시뮬레이션 뉴욕 메츠의 대부분의 경기가 지명 타자가 없는 내셔널 리그라는 점 뉴욕 양키스는 대부분 지명타자가 있는 아메리칸 리그 경기를 한다는 점 고려하지 X [첫번째 시뮬레이션] 분석 대상의 실점은 고려하지 X 득점에만 초점을 둔 경우 1234567891011121314151617# 파일 읽어오기# 깨지는 헤더 수정library(reshape)mets &lt;- read.csv(\"./data/NY_m.csv\",header=TRUE,encoding = \"UTF-8\")mets &lt;- rename(mets, c(\"X.U.FEFF.hometeam\" = \"hometeam\"))yankees &lt;- read.csv(\"./data/NY_y.csv\",header=TRUE,encoding = \"UTF-8\")yankees &lt;- rename(yankees, c(\"X.U.FEFF.hometeam\" = \"hometeam\"))# 원정팀 메츠의 득점과 홈팀 양키스의 득점away_mets &lt;- mets$away_NY_maway_mets &lt;- away_mets[-c(32:34)] # 결측값과 평균값 삭제 위함home_yankees &lt;- yankees$home_NY_yhome_yankees &lt;- home_yankees[-c(32:34)] away_mets와 home_yankees의 득점값을 랜덤으로 추출 가상 게임을 돌림 (랜덤으로 추출된 숫자 비교) 동점일 경우 카운트하지 X 123456789101112131415161718192021222324# 득점 값 랜덤 추출random_mets &lt;- sample(away_mets, 50000, replace=TRUE)random_yankees &lt;- sample(home_yankees, 50000, replace=TRUE)# 각 팀이 몇번 승리하였는지 카운트 할 변수mets_win_cnt = 0yankees_win_cnt = 0for(i in 1:length(random_mets))&#123; if (random_mets[i] &gt; random_yankees[i]) mets_win_cnt &lt;- mets_win_cnt + 1 else if(random_mets[i] &lt; random_yankees[i]) yankees_win_cnt &lt;- yankees_win_cnt + 1 else next&#125;# 승률mets_win_rate = mets_win_cnt/length(random_mets)yankees_win_rate = yankees_win_cnt/length(random_mets)cat('뉴욕 메츠의 승률 : ', mets_win_rate,'\\n')cat('뉴욕 양키스의 승률 : ', yankees_win_rate) [두번째 시뮬레이션] 공격과 수비 모두 고려 (득점과 실점) - 공격 득점(대상팀의 득점)과 수비 실점(상대팀의 득점) - 각 팀의 예상 득점을 추정하기 위해서는,해당 팀이 획득한 득점과 허용한 실점에 대한경험적 분포에서 추출한 임의의 랜덤 값을 사용 - 상대팀의 공격과 수비 숫자를 평균 메츠와 양키스가 서로 상대로 한 게임을 분석하기 위해서는, 공격과 수비 성적 데이터를 모두 고려해야 함! 양키스 경기장에서 열린 게임에서원정팀인 뉴욕 메츠는 홈팀인 양키스를 대상으로 공격홈팀 양키스는 워정팀 메츠를 대상으로 공격 1234567891011121314# 메츠의 득점과 실점away_mets &lt;- mets$away_NY_maway_mets &lt;- away_mets[-c(32:34)]away_mets_runs &lt;- mets$home_scoreaway_mets_runs &lt;- away_mets_runs[-c(32:34)]awaymets_for_graph &lt;- data.frame(away_mets,away_mets_runs)# 양키스 득점과 실점home_yankees &lt;- yankees$home_NY_yhome_yankees &lt;- home_yankees[-c(32:34)]home_yankees_runs &lt;- yankees$home_scorehome_yankees_runs &lt;- home_yankees_runs[-c(32:34)]homeyankees_for_graph &lt;- data.frame(home_yankees,home_yankees_runs) ggplot 라이브러리 사용해 공격과 수비를 모두 고려한 메츠 원정팀, 양키스 홈팀의 득실 빈도 그래프를 그려보면 1234567891011121314151617181920212223242526272829303132333435# 메츠의 득점 요인random_mets_attack &lt;- sample(away_mets, 50000, replace=TRUE)random_mets_defence &lt;- sample(away_mets_runs, 50000, replace=TRUE)avg_mets &lt;- 0# 양키스의 득점 요인random_yankees_attack &lt;- sample(home_yankees, 50000, replace=TRUE)random_yankees_defence &lt;- sample(home_yankees_runs, 50000, replace=TRUE)avg_yankees &lt;- 0for(i in 1:length(random_mets_attack))&#123;avg_mets &lt;- append((random_mets_attack[i]+random_mets_defence[i])/2,avg_mets)avg_yankees &lt;- append((random_yankees_attack[i]+random_yankees_defence)/2,avg_yankees)&#125;# 각 팀이 몇번 승리하였는지 카운트 할 변수mets_win_cnt = 0yankees_win_cnt = 0for(i in 1:length(avg_mets))&#123; if (avg_mets[i] &gt; avg_yankees[i]) mets_win_cnt &lt;- mets_win_cnt + 1 else if(avg_mets[i] &lt; avg_yankees[i]) yankees_win_cnt &lt;- yankees_win_cnt + 1 else next&#125;# 승률mets_win_rate = mets_win_cnt/length(avg_mets)yankees_win_rate = yankees_win_cnt/length(avg_mets)cat('뉴욕 메츠의 승률 : ', mets_win_rate,'\\n')cat('뉴욕 양키스의 승률 : ', yankees_win_rate)","categories":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/"},{"name":"Modeling","slug":"BIGDATA/Modeling","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/Modeling/"},{"name":"R","slug":"BIGDATA/Modeling/R","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/Modeling/R/"}],"tags":[{"name":"modeling","slug":"modeling","permalink":"https://NAEJINHJ.github.com/tags/modeling/"},{"name":"R","slug":"R","permalink":"https://NAEJINHJ.github.com/tags/R/"}]},{"title":"NoSQL이란?","slug":"NoSQL","date":"2019-01-24T07:15:52.142Z","updated":"2019-02-08T13:39:08.088Z","comments":true,"path":"2019/01/24/NoSQL/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/24/NoSQL/","excerpt":"","text":"[목차] 개념 특징 장점 단점 종류 MongoDB EcoSystem 1. 개념 Not Only SQL의 약자 기본 RDBMS의 한계를 극복하기 위해 만들어진 새로운 형태의 데이터베이스 분산 환경에서 대용량의 데이터를 빠르게 처리하기 위해 개발 됨 분산형 구조를 통해 여러대의 서버에 분산하여 저장하고, 상호 복제하여 데이터 유실이나 서비스 중지에 대비 핵심은 Horizontal Scalability(수평확장)과 High Availability(고가용성) 릴레이션이 아니므로 고정된 스키마가 없고(Schema-less) 조인이 힘듦 거대한 Map으로서 key-value 형식을 지원 대부분 CAP 이론을 따름 CAP 이론 [Consistency]-일관성 분산된 노드 중, 어느 노드로 접근하더라도 데이터 값이 같아야 함 (데이터 복제 중에 쿼리가 되는 시스템, 즉 일관성을 제공하지 않는 시스템의 경우 다른 데이터 값이 쿼리 될 수 있음) [Availability]-가용성 클러스터링된 노드 중 하나 이상의 노드가 실패라도 정상적으로 요청을 처리할 수 있는 기능을 제공 [Partition Tolerance]-분산 허용 클러스터링 노드 간에 통신하는 네트워크가 장애가 나더라도 정상적으로 서비스를 수행 노드간 물리적으로 전혀 다른 네트워크 공간에 위치도 가능 이 중 2가지만 만족할 수 있음 - RDBMS는 일반적으로 일관성과 가용성 만족 - NoSQL [가용성/분산허용 만족] or [일관성/분산허용 만족] 제품군으로 나뉨 [RDBMS의 한계] 많은 데이터양과 처리량이 계속적으로 증가한다면, ● 스키마 문제: 빅데이터를 RDB의 스키마에 맞춰 변경해서 넣으려면 매우 긴 시간의 down time 발생 ● 스케일업의 한계: 관계 모델과 트랜잭션의 연산, 일관성, 속성을 유지하면서 분산환경(스케일 아웃)에서 RDBMS를 조작하는 것은 어려움 2. 특징 [Document-oriented storage] MongoDB는 database &gt; collections &gt; documents 구조 document는 key-value 형태의 BSON(Binary JSON) [Full Index Support] 다양한 인덱싱 제공 Single Field Indexes Compound Indexes Multikey Indexes Geospatial Indexes and Queries Text Indexes Hashed Index [Replication &amp; High Availability] 간단한 설정만으로 데이터 복제 지원, 가용성 향상 [Auto-Sharding] 처음부터 자동으로 데이터를 분산하여 저장 하나의 컬렉션처럼 사용 가능 수평적 확장 가능 [Querying](documented-based query) 다양한 종류의 쿼리문 지원 (필터링, 수집, 정렬, 정규표현식) [Fast In-Pace Updates] 고성능의 atomic operation 지원 [Map/Reduce] 분산/병렬 시스템 운용 지원 [GridFS] 자동으로 분산 파일 저장 실제 파일이 어디에 저장되어 있는지 신경쓸 필요 X 복구 역시 자동 [Commercial Support] 10gen에서 관리하는 오픈 소스 3. 장점 ● 클라우드 컴퓨팅 환경에 적합 Open Source임 하드웨어 확장에 유연한 대처가 가능 RDBMS에 비해 저렴한 비용으로 분산 처리와 병렬 처리 가능 ● 유연한 데이터 모델임 비정형 데이터 구조 설계로 설계 비용 감소 관계형 데이터베이스의 Relationship과 Join구조를 Linking과 Embedded로 구현해 성능이 빠름 (join을 피할 수 있기 때문) ● 빅데이터 처리에 효과적 Memory Mapping 기능을 통해 Read/Write가 빠름 기존의 OS와 H/W에 구축 가능 기존 RDB와 동일하게 데이터 처리 가능 4. 단점 ● 정합성이 떨어지므로 트랜잭션이 필요한 경우에는 부적합 (ex. 금융, 결제 등) ● JOIN이 없기 때문에, join이 필요없도록 데이터 구조화 필요 ● memory mapped file으로 파일 엔진 DB 메모리 관리를 OS에 위임메모리에 의존적, 메모리 크기가 성능 좌우 ● SQL 완전히 이전 불가 ● B트리 인덱스 사용해 인덱스 생성, 크기가 커질 수록 입력/삭제 성능 저하 5. 종류 MongoDB Casandra HBASE CouchDB Riak Redis [참조 사이트] http://nosql-database.org http://www.mongodb.org http://hbase.apache.org/ http://couchdb.apache.org/ 4. MongoDB EcoSystem [Sharding System] 빅데이터의 효율적인 분산 저장 [ReplicaSets] 데이터의 안전한 복제 저장 [Text Search Engine / Full Indexing 기법] 저장된 데이터를 보다 빠르게 추출 [MapReduce / Aggregation FrameWork] 데이터의 빠른 분석, 가공처리 [GridFs] 비정형 데이터의 효율적인 저장 및 관리 [OPS Manager / Compass] 각 Sub System이 효율적으로 운영되고 있는지 모니터링 [참조] MongoDB Master가 해설하는 New NoSQL &amp; mongoDB - 주종면 저 NoSQL (개념, 특징과 장점, CAP 이론, 데이터모델 분류) 출처: https://sjh836.tistory.com/97 [빨간색코딩]","categories":[{"name":"DB","slug":"DB","permalink":"https://NAEJINHJ.github.com/categories/DB/"},{"name":"NoSQL","slug":"DB/NoSQL","permalink":"https://NAEJINHJ.github.com/categories/DB/NoSQL/"}],"tags":[{"name":"DB","slug":"DB","permalink":"https://NAEJINHJ.github.com/tags/DB/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://NAEJINHJ.github.com/tags/NoSQL/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://NAEJINHJ.github.com/tags/MongoDB/"}]},{"title":"124 나라의 숫자","slug":"country124","date":"2019-01-19T09:33:35.307Z","updated":"2019-01-19T14:04:53.098Z","comments":true,"path":"2019/01/19/country124/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/19/country124/","excerpt":"","text":"n%3 == 0 n%3 == 1 n%3 == 2 4 1 2 파라미터 n을 3으로 나눈 나머지 (n%3) n % 3 == 0 인 경우는 예외 처리해야 함 (n=15일 경우) &gt; 이 경우 n-1 값을 n으로! 나머지 값을 인덱스로 리스트에 접근하기 위해, 리스트의 값은 [4,1,2]로 설정했음 (python 3.x 버전은 나눗셈 연산으로 //을 쓸 것!) 123456789101112def solution(n):answer = ''arr = [4, 1, 2]rem = 0while(n&gt;0): rem = n % 3 n = n//3 if rem == 0: n -= 1 answer = str(arr[rem]) + answerreturn answer","categories":[{"name":"Coding Test","slug":"Coding-Test","permalink":"https://NAEJINHJ.github.com/categories/Coding-Test/"},{"name":"Python","slug":"Coding-Test/Python","permalink":"https://NAEJINHJ.github.com/categories/Coding-Test/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"coding test","slug":"coding-test","permalink":"https://NAEJINHJ.github.com/tags/coding-test/"}]},{"title":"A+ 스토리","slug":"java-project","date":"2019-01-18T02:09:28.522Z","updated":"2019-02-10T16:02:38.393Z","comments":true,"path":"2019/01/18/java-project/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/18/java-project/","excerpt":"","text":"[목차] UML MAIN CHARACTER PLAY TRAP ENDING ◆ UML ◆ MAIN ◆ CHARACTER ◆ PLAY ◆ TRAP ◆ ENDING","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"JAVA","slug":"Project/JAVA","permalink":"https://NAEJINHJ.github.com/categories/Project/JAVA/"}],"tags":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/tags/Project/"},{"name":"JAVA","slug":"JAVA","permalink":"https://NAEJINHJ.github.com/tags/JAVA/"},{"name":"game","slug":"game","permalink":"https://NAEJINHJ.github.com/tags/game/"}]},{"title":"고 & 스톱","slug":"C-sharp","date":"2019-01-18T02:09:10.727Z","updated":"2019-01-18T08:26:59.914Z","comments":true,"path":"2019/01/18/C-sharp/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/18/C-sharp/","excerpt":"","text":"[목차] 1. HOME 2. HOW TO 3. GAME PLAY 4. GO STOP 5. SCORE 특이 사항 1. HOME [START button] MouseMove 이벤트: 버튼에 마우스 올리면, 웃고 있는 캐릭터 이미지로 변경 MouseLeave 이벤트: 마우스가 버튼 밖으로 벗어나면, 다시 원래의 이미지로 돌아감 Click 이벤트: 게임 플레이 화면으로 전환 [HowTo button] MouseMove 이벤트: 버튼에 마우스 올리면, 웃고 있는 캐릭터 이미지로 변경 MouseLeave 이벤트: 마우스가 버튼 밖으로 벗어나면, 다시 원래의 이미지로 돌아감 Click 이벤트: 게임 룰 설명 화면으로 전환 [화면 전환 방식] 1) HOME 폼을 숨김 2) 실행하고 싶은 폼의 객체 생성하여 실행 시킴 3) HOME 폼 닫음 2. HOW TO [룰 설명] 이미지로 제작, backgroundimage로 설정 [Click 이벤트] 화살표 버튼 클릭 시, 다시 홈 화면으로 전환 3. GAME PLAY 총 5개의 패널로 구성 [computerCardPanel] 위에서 부터 컴퓨터 패를 놓음 [computerScorePanel] 컴퓨터가 얻은 카드 순서대로 정렬 점수 출력 GO를 한 횟수를 출력 [playPanel] 실제 게임의 필드(바닥)에 해당하는 패널 [UserScorePanel] 유저가 얻은 카드 순서대로 정렬 점수 출력 GO를 한 횟수를 출력 [UserCardPanel] 유저의 카드 패를 놓음 카드 패는 버튼으로 구성 [Card Setting] 카드 클래스 구현 생성자에서 카드의 인덱스, 이미지, 월과 가치 값 세팅 크기 48의 클래스 배열 생성 카드를 순서대로 저장시킨 뒤, 랜덤하게 섞어줌 분배 0~9 : 유저의 카드패 10~ 19 : 컴퓨터의 카드패 20 ~ 27: 필드에 깔리는 카드 28 ~ 47: 뒤집어져 있는 카드 더미 [이미지 예외 처리] 이미지 세팅 과정에서 필요한 이미지가 존재하지 않을 시, 예외 처리 에러 실행 [GAME START] 필드에 카드가 랜덤하게 8장 깔림 (필드에 깔린 카드들은 리스트로 관리) Card Setting 과정에서 분배받은 8장의 카드에 대해, picturebox 생성하는 식으로 구현 동일한 월을 갖는 카드가 있을 시, 일정한 간격으로 겹쳐지게 함 (각 월마다 기준 좌표가 존재) [User’s Card] 10개의 버튼(유저패)의 backgroundimage 분배받은 카드의 이미지 버튼 클릭 시, 유저가 그 카드를 낸 것으로 간주 해당 버튼의 사용을 금지시키고, 더이상 보이지 않도록 설정 [총통] 유저 혹은 상대 패에 같은 월의 카드가 4장 있을 시, 총통임을 메시지 박스로 보여주고, 즉시 50점 획득해 승리하도록 함 [User’s turn] 필드에 일치하는 월이 있는지, 해당 월의 카드가 몇 장 있는지 체크 조건에 따라 득점되어 필드에 추가 or 똥 처리 득점한 카드 가치 값에 따라 다른 좌표로 정렬되어 각 score 패널에 표시 이후 상대의 turn으로 바뀜 [Computer’s turn] 낸 카드는 숨겨짐 delay 함수 사용 카드를 내거나 이벤트가 발생되었을 때, 육안으로 잘 보이게 하기 위함 (시간 지연 함수) [Computer’s turn] 가진 패의 앞에서부터 카드를 내도록 함 현재 점수와 고의 횟수에 대한 정보가 각 스코어 패널의 레이블에 표시 됨 [똥 Event] 더미에서 카드를 뒤집고 3장이 겹치게 되었을 때 발생 &gt; 똥 이미지 출력, 3장의 카드는 그대로 필드에 남김 (만약 필드에 남은 3장의 카드와 같은 월의 카드를 낸다면 총 4장으 얻게 됨) [Score] 각 피의 개수에 따른 점수 홍단 / 청단 / 초단 고도리 3광 / 4광 / 5광 피박 고박 고 횟수 당 1점 추가 [Calculating] 리스트로 상대와 내가 획득한 카드의 인덱스 저장해, 이를 토대로 점수 계산 이벤트 발생 시, (홍단/청단/초단, 고도리, 3광/4광/5광, 똥) 델리게이트를 사용 이미지 출력과 점수 계산이 되도록 구현 [Game Over] 남은 더미 카드 수 소진 스탑 선택 시 4. GO STOP GO의 여부를 묻는 폼 7점 이상 획득 시 생성 7점 이하까지는 피박이 적용되지 X 카드가 추가되어 면박이 될 경우, 점수가 하락하지만 GO/STOP 선택은 가능 [GO 선택] GO 이미지가 뜸 점수와 GO의 횟수 하나씩 증가 게임 계속 진행 [STOP 선택] 유저의 승리로 게임 종료 게임 종료 시, 스코어폼으로 화면 전환 [선택 결과 전달] 어떠한 버튼을 선택했는지에 대한 정보는 DialogResult를 통해 게임플레이 폼으로 전달 어떤 버튼을 선택했는지에 따라 다른 이미지가 띄워짐 5. SCORE 2개의 패널로 구성 (컴퓨터용 / 유저용) [게임 결과] 색으로 승패 표시 캐릭터 표정 &amp; 도장 이미지 승자의 패널에는 점수와 획득 금액 출력 (무승부 시, 우측과 같은 화면 띄워짐) [특이 사항] - 보너스 패, 흔들기 등을 생략해 프로젝트 스케일 조정 - 게임에 사용된 이미지 직접 제작 - 버전별 업데이트 내역 관리하여 효율적인 작업 수행","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"C#","slug":"Project/C","permalink":"https://NAEJINHJ.github.com/categories/Project/C/"}],"tags":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/tags/Project/"},{"name":"game","slug":"game","permalink":"https://NAEJINHJ.github.com/tags/game/"},{"name":"C#","slug":"C","permalink":"https://NAEJINHJ.github.com/tags/C/"}]},{"title":"Titanic: Machine Learning from Disaster - first challenge","slug":"Titanic","date":"2019-01-13T13:20:04.365Z","updated":"2019-02-26T06:14:10.234Z","comments":true,"path":"2019/01/13/Titanic/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/13/Titanic/","excerpt":"","text":"[Element] Survival NO = 0 Yes = 1 Pclass Ticket class 1st = 1 2nd = 2 3rd = 3 SibSp 동반한 형제, 자매, 배우자의 수 Parch 동반한 부모, 자식의 수 ticket 티켓 고유 넘버 Fare Passenger fare Cabin 객실 번호 Embarked 출발지 Cherbourg = C Queenstown = Q Southampton = S [향후 개선 방안] 이름의 last name 추출해 가족을 그룹으로 식별해볼 것 Age 결측치 다른 값으로 채워보기 Partner, sex, age를 고려 → 30대 이상일 경우, 부모일 확률 높음 참고해 볼 곳 &gt;&gt; https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling https://towardsdatascience.com/kaggle-titanic-machine-learning-model-top-7-fa4523b7c40 https://towardsdatascience.com/how-i-got-98-prediction-accuracy-with-kaggles-titanic-competition-ad24afed01fc","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"Kaggle","slug":"Project/Kaggle","permalink":"https://NAEJINHJ.github.com/categories/Project/Kaggle/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"modeling","slug":"modeling","permalink":"https://NAEJINHJ.github.com/tags/modeling/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://NAEJINHJ.github.com/tags/Kaggle/"}]},{"title":"수소 충전소 최적의 설립 위치 추천\n- ➂ 분석 결과","slug":"dongguk-final","date":"2019-01-13T08:50:07.339Z","updated":"2019-01-13T09:07:17.871Z","comments":true,"path":"2019/01/13/dongguk-final/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/13/dongguk-final/","excerpt":"","text":"[목차] 분석요인 별 추천 입지 수소 충전소 최종 입지 추천 활용방안 및 한계점 참고 문헌 [분석요인 별 추천 입지] [수소 충전소 최종 입지 추천] [상위 5 곳] [상위 20 곳] [활용방안 및 한계점] [참고 문헌]","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"BIGDATA","slug":"Project/BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/Project/BIGDATA/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"modeling","slug":"modeling","permalink":"https://NAEJINHJ.github.com/tags/modeling/"},{"name":"R","slug":"R","permalink":"https://NAEJINHJ.github.com/tags/R/"},{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/tags/Project/"}]},{"title":"수소 충전소 최적의 설립 위치 추천\n- ② 데이터 분석","slug":"dongguk2","date":"2019-01-10T12:07:57.693Z","updated":"2019-01-13T08:42:51.238Z","comments":true,"path":"2019/01/10/dongguk2/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/10/dongguk2/","excerpt":"","text":"[목차] 데이터 목록 및 전처리 분석 방법 - AHP 개요 - 의사결정 계층화 - 요소 선정 - 평가 기준 비교 - 가중치 추정 - 일관성 검증 [데이터 목록 및 전처리] 1. 사용 데이터 목록2. 전처리 과정 csv 형태의 원본 데이터를 python pandas를 이용, dataframe 형태로 가져옴 장소명과 도로명 주소 데이터 추출 지오코딩 API 이용 &gt;&gt; 해당 장소의 좌표값(위도, 경도) 얻음 하버사인 공식 사용 &gt;&gt; 좌표값을 기반으로, 곡면에서 두 지점간의 거리 계산 [분석 방법 - AHP] 1. AHP란? 고려한 데이터가 대부분 거리 데이터였고, 타 논문 참고 결과, 입지 선정시 AHP 기법을 가장 많이 활용한 것을 확인 2. 의사결정 계층- 분석 기준 설정 [상위 단계] 최종 목표(overall goal) [Level.1 단계] 평가 기준 1) 안정성 2) 설립 비용 3) 운영 효율성 [Level.2 단계] 평가 기준에 대한 선택 대안 1) 제 1 보호지역과의 거리 2) 서울 자치구별 화재 발생 3) 2km 반경 내 타 LPG 충전소 존재 유무 4) 버스 차고지와의 거리 3. 요소 선정 [안정성] 1) 제 1종 보호시설과의 거리 주유소의 폭발 위험성을 고려하여, 제 1 보호시설 간의 거리를 분석 요소로 선정 (초중고,유치원 데이터 이용) 제 1종 보호시설과 LPG 주유소간의 거리가 멀수록, 가중치 값 더 크게 산정 2) 자치구별 화재 발생률 수소의 경우, 타가스에 폭발했을 시 [위험 피해율]이 크고, 수소 위험성에 대한 인식이 수소 충전소 설립에 큰 영향을 미칠 수 있기에 분석 요소로 선정하였음 [설립 비용] 2km 반경 타 LPG 충전소 존재 유무 LPG 주유소가 밀집되어 있다면, 밀집된 LPG 주유소 중 한 곳을 수소 충전소로 대체하는 방안 고려 [운영 효율성] 버스 차고지와의 거리 2027년까지, 수도권의 경유 버스 → 수소/전기 버스 등으로 전면 교체 향후 버스 차고지 근처에 수소 충전소가 설립될 경우, 충전소 운영에 영향을 미칠 것이라 판단 매일 경제 (http://news.mk.co.kr/,2018.05.14) 월간 수소 경제 (http://www.h2news.kr/2018.07.10) 4. 평가 기준 비교 앞서 선정한 4가지 요인들에 대해, 중앙값을 기준으로 4개의 그룹으로 나눠, 그룹 번호 부여 모든 것들이 다 [커지면 좋거나/ 작아지면 좋거나]의 기준 이었기 때문에, 중앙값을 기준으로 grouping하기로 판단함 (타 논문을 참고하여, 쌍대비교치 3,5,7로 설정) 대안별 중요 순위는 주관적으로 설정됨 검증을 위해 이후 일관성 검증을 진행하였음 [쌍대비교표 설명] 차고지 기준으로 화재는 매우 중요 &gt;&gt; 7 부여 1종 보호시설은 화재보다는 덜 중요 즉, 적당히 중요 &gt;&gt; 5 부여 차고지 기준 2km 이내의 타 충전소의 개수는 약간 중요 &gt;&gt; 3 부여 [쌍대비교 결과]- 버스차고지 구별 화재 발생률, 제 1종 보호시설, 주변 LPG 주유소 요인에 대해서도 동일하게 쌍대비교 수행 5. 가중치 추정 고유치 방법(Eigenvalue Method) 쌍별 비교된 의사 결정 요소들 간의 쌍대적 가중치 계산 k = 평가 기준 (버스 차고지, LPG 주유소 밀집도, 제 1종 보호시설, 자치구별 화재 비율) 상단의 수식을 통해 최종 행렬(nX1)을 만들어 냄 6. 일관성 검증 상단의 수식을 통해 일관성 검증 수행","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"BIGDATA","slug":"Project/BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/Project/BIGDATA/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"modeling","slug":"modeling","permalink":"https://NAEJINHJ.github.com/tags/modeling/"},{"name":"R","slug":"R","permalink":"https://NAEJINHJ.github.com/tags/R/"},{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/tags/Project/"}]},{"title":"수소 충전소 최적의 설립 위치 추천\n- ① 분석 배경 및 목표","slug":"dongguk","date":"2019-01-10T02:29:50.835Z","updated":"2019-01-12T07:23:51.605Z","comments":true,"path":"2019/01/10/dongguk/","link":"","permalink":"https://NAEJINHJ.github.com/2019/01/10/dongguk/","excerpt":"","text":"[목차] 분석 배경 분석 목표 [개요] ▶ 빅데이터 청년인재 [빅데이터 분석 기반 지능 SW 과정] 프로젝트 ▷ 기간 2018/07/01 ~ 2018/09/07 (약 2개월) ▶ 담당 역할 데이터 수집 데이터 전처리 모델링 ▷ 활용 분석 도구 Jupyter notebook (python) Folium (python) Rstudio (R) Excel 1. 분석 배경 ▶ 목표치에 따른 수소 충전소 확충 진행률의 부진 확인가능 “부지 확보 장기간 소요” &gt;&gt; 주요한 수소 충전소 보급 지연 원인 ▶ 일반 전기차 충전소의 약 20배의 비용 소요 ▷ 기존 충전소 / 주유소 부지 활용해 최적의 배치 전략 필요 [출처] news1뉴스(http://news1.kr,2018.03.22) 수소 산업 협회는 서울에 한정 짓더라도 기존의 LPG 충전소를 활용하면, 약 70여곳의 수소 충전소 확보가 가능하다 밝힘 기존의 부지, 건물, 인력을 그대로 활용하는 것이 비용적인 측면에서 효율적인 것을 확인 가능 [기타 참조 가능 기사] - LPG-수소 융‧복합 충전소, 부지 면적 확보가 ‘핵심’ (http://www.gnetimes.co.kr/news/articleView.html?idxno=44757) - 1145개 LPG충전ㆍ주유소 “수소 복합 충전소 전환 가능” (http://www.energy-news.co.kr/news/articleView.html?idxno=54052) 2. 분석 목표","categories":[{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/categories/Project/"},{"name":"BIGDATA","slug":"Project/BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/Project/BIGDATA/"}],"tags":[{"name":"python","slug":"python","permalink":"https://NAEJINHJ.github.com/tags/python/"},{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/tags/BIGDATA/"},{"name":"modeling","slug":"modeling","permalink":"https://NAEJINHJ.github.com/tags/modeling/"},{"name":"R","slug":"R","permalink":"https://NAEJINHJ.github.com/tags/R/"},{"name":"Project","slug":"Project","permalink":"https://NAEJINHJ.github.com/tags/Project/"}]},{"title":"LA Dodgers 홍보 효과 분석","slug":"bobblehead","date":"2018-12-20T14:22:06.015Z","updated":"2019-01-27T17:28:04.285Z","comments":true,"path":"2018/12/20/bobblehead/","link":"","permalink":"https://NAEJINHJ.github.com/2018/12/20/bobblehead/","excerpt":"","text":"● 목적 1) 버블헤드 인형 홍보전략을 통한 야구장 관중 증가 여부 2) 야구표로 증가한 구단 수익으로 버블헤드 인형 홍보활동에 투자한 고정비용, 가변비용 충당 가능 여부 ● 라이브러리 1) 선형 회귀 분석용 함수: car 2) 그래픽 패키지: lattice &lt;메이저리그 2012년 다저스 홈경기 데이터&gt; ordered_day_of_week 정렬된 요일 변수 ordered_month 정렬된 월 변수 [요일별 dodgers 관중수 분포] 123with(data=dodgers,plot(ordered_day_of_week,attend/1000,xlab=\"Day of Week\",ylab = \"Attendence (thousands)\",col=\"violet\",las=1)) [월별 dodgers 관중수 분포] 123with(data=dodgers,plot(ordered_month,attend/1000,xlab=\"Month\",ylab=\"Attendance (thousands)\",col=\"light blue\",las=1)) [주간/야간 경기 &amp; 불꽃놀이 유무에 따른 관중수 분포] 경기 시간과 날씨 (맑음/흐림)에 대한 조건을 설정 → 온도와 관중수 간의 관계 파악 * library(lattice) (참조: http://visualize.tistory.com/305): 직교 형태의 그래픽(Trellis graphic)을 생성하는 코드 포함1) 다차원의 데이터를 사용하려 할 때, 한번에 많은 플롯 생성 가능2) 기본 플로팅 시스템의 방법을 ‘mfrow’와 ‘mfcall’이라는 인수 통해 활용 가능 1234567891011121314151617181920212223library(lattice) # 플로팅을 위해 사용하는 라이브러리group.labels &lt;- c(\"No Fireworks\",\"Fireworks\")group.symbols &lt;- c(21,24)group.colors &lt;- c(\"black\",\"black\")group.fill &lt;- c(\"black\",\"red\")xyplot(attend/1000~temp | skies + day_night, data=dodgers, groups=fireworks, pch=group.symbols, aspect= 1, cex=1. 악 * library(&lt;font color=5, col=group.colors, fill=group.fill, layout=c(2,2), type=c(\"p\",\"g\"), strip=strip.custom(strip.levels=TRUE,strip.names=FALSE, style=1), xlab = \"Temperature (Degrees Fahrenheit)\", ylab = \"Attendance (thousands)\", key = list(space=\"top\", text = list(rev(group.labels), col=rev(group.colors)), points= list(pch=rev(group.symbols),col=rev(group.colors), fill = rev(group.fill)) ) #list ) # xyplot &gt; 그래프 분석 결과, 맑은 날씨의 주간경기에는 온도가 높을수록 관중수가 적다.&gt; 일반적으로 일요일에는 주간경기를 한다.&gt; 2012년 LA 다저스 구장의 경기는 한 게임을 제외하고 모두 날씨가 좋았다. [LA 다저스의 상대팀 기준 다저스 구장 관중수] 단변량 산점도12345678910111213141516group.labels &lt;- c(\"Day\",\"Night\")group.symbols &lt;- c(1,20)group.symbols.size &lt;- c(2,2.75)bwplot(opponent~attend/1000,data=dodgers, groups=day_night, xlab = \"Attendance (thousands)\", panel = function(x,y,groups,subscripts, ...)&#123; panel.grid(h= (length(levels(dodgers$opponent))-1),v=-1) panel.stripplot(x,y,groups=groups, subscripts = subscripts, cex=group.symbols.size, pch=group.symbols, col=\"darkblue\" ) &#125;, key= list(space=\"top\", text=list(group.labels,col=\"black\"), points = list(pch=group.symbols, cex=group.symbols.size, col=\"darkblue\") ) )&gt; 도시 규모가 큰 메트로폴리탄 지역을 연고로 하는 팀과 경기를 하는 경우 관중수는 큰 변화 X [회귀모형] train-test 모형- 버블 헤드 인형을 이용한 홍보활동에 대한 통계적 유의성 검증- 유형I 분산 분석으로 순차적 검증에 대한 제곱합을 계산1print(anova(my.model.fit))진단용 plot [패키지 car 이용해 추가 모형 진단] 123library(car)residualPlots(my.model.fit)marginalModelPlots(my.model.fit) 유의한 outlier 확인1print(outlierTest(my.model.fit))&gt; 없는 것으로 확인&lt;다저스의 버블헤드 인형 홍보에 대한 예측모형에 근거&gt;1) 다가오는 시즌에 예측 대상 경기의 관중수 예측 가능2) 버블헤드 인형을 활용한 홍보에 따른 관중수 예측 가능3) 관중 예측값 사용해 홍보 유무에 따른 다저스 수익 예측 가능","categories":[{"name":"BIGDATA","slug":"BIGDATA","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/"},{"name":"Modeling","slug":"BIGDATA/Modeling","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/Modeling/"},{"name":"R","slug":"BIGDATA/Modeling/R","permalink":"https://NAEJINHJ.github.com/categories/BIGDATA/Modeling/R/"}],"tags":[{"name":"modeling","slug":"modeling","permalink":"https://NAEJINHJ.github.com/tags/modeling/"},{"name":"R","slug":"R","permalink":"https://NAEJINHJ.github.com/tags/R/"}]},{"title":"START","slug":"START","date":"2018-12-08T06:24:14.436Z","updated":"2018-12-08T06:26:35.811Z","comments":true,"path":"2018/12/08/START/","link":"","permalink":"https://NAEJINHJ.github.com/2018/12/08/START/","excerpt":"","text":"","categories":[{"name":"LOG","slug":"LOG","permalink":"https://NAEJINHJ.github.com/categories/LOG/"}],"tags":[{"name":"log","slug":"log","permalink":"https://NAEJINHJ.github.com/tags/log/"},{"name":"test","slug":"test","permalink":"https://NAEJINHJ.github.com/tags/test/"}]}]}